{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f369f3d3-2a27-4153-9994-61e8a56c91bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse using notebook for now, don't need these\n",
    "# import dill\n",
    "import jax\n",
    "import dill\n",
    "import jax.numpy as np\n",
    "from jax.scipy.special import logsumexp\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0515c2-5a81-4b8c-ad27-351977dd8ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNS=1\n",
    "input_dir = \"../Datasets\"\n",
    "algos = [\"optimistic\", \"softmax\", \"igw\", \"greedy\", \"ucb\", \"ts\"]\n",
    "\n",
    "filename = f'{input_dir}/dataset_1_ucb.dill'\n",
    "with open(filename, 'rb') as f:\n",
    "    data = dill.load(f)\n",
    "            \n",
    "    data_x = np.array(data['x'])  \n",
    "    data_a = np.array(data['a'])  \n",
    "#    rhox = data['rhox']           \n",
    "#    betas_mean = data['betas_mean']\n",
    "#    betas_cov=data['betas_cov']\n",
    "    T,A,K = data_x.shape    \n",
    "\n",
    "# A dictionary of hyperparameters of the simulation\n",
    "hyper = dict()\n",
    " \n",
    "############### ADJUST HYPER AND THE LOADING\n",
    "hyper['n_samples'] = 100\n",
    "hyper['iter'] = 100\n",
    "\n",
    "T = data_x.shape[0] # time dimension, total number of steps\n",
    "A = data_x.shape[1] # this is not used anywhere !!! Each action has it's own context, this is unusual, interesting setup\n",
    "K = data_x.shape[2] # feature space dimension, context shape\n",
    "alpha = 20 # exploration parameter in the policy definition\n",
    "sigma = .10 # variance of the rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabfe985-88c2-4c08-ae00-ad5c316f86a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Belief calculation utilities\n",
    "\n",
    "## cumulative sum of outer products of contexts selected by actions up to time t\n",
    "## later used for covariance matrix updates\n",
    "\n",
    "__betas_N = lambda t: np.einsum('i,j->ij', data_x[t,data_a[t]], data_x[t,data_a[t]])\n",
    "__betas_N = jax.vmap(__betas_N)\n",
    "_betas_N = __betas_N(np.arange(T-1)).cumsum(axis=0)\n",
    "_betas_N = np.concatenate((np.zeros((K,K))[None,...], _betas_N))\n",
    "\n",
    "__betas_y = lambda r, t: r * data_x[t,data_a[t]]\n",
    "__betas_y = jax.vmap(__betas_y)\n",
    "_betas_y = lambda rs: np.concatenate((np.zeros(K)[None,...], __betas_y(rs, np.arange(T-1)).cumsum(axis=0)))\n",
    "_betas_y = jax.jit(_betas_y)\n",
    "_BETAS_Y = jax.jit(jax.vmap(_betas_y))\n",
    "\n",
    "@jax.jit\n",
    "def decode(params):\n",
    "    '''\n",
    "    initialize or unpack parameters \n",
    "    '''\n",
    "    beta0 = np.exp(20 * params['beta0']) \n",
    "    beta0_y = -np.ones(K)/K * beta0  # vector\n",
    "    beta0_N = np.eye(K) * beta0      # matrix\n",
    "    return beta0_y, beta0_N\n",
    "\n",
    "# Implement likelihood functions for each policy\n",
    "def optimistic_pi(beta_mean, x, beta_cov, a):\n",
    "    q = alpha * np.einsum('ij,j->i', x, beta_mean) + np.einsum('ij,jk,ki->i', x, beta_cov, x.T)\n",
    "    return q[a] - logsumexp(q)    \n",
    "\n",
    "def softmax_pi(beta_mean, x, beta_cov, a):\n",
    "    q = alpha * np.einsum('ij,j->i', x, beta_mean)\n",
    "    return q[a] - logsumexp(q) \n",
    "\n",
    "def ts_pi(beta_mean, x, beta_cov, a, key):\n",
    "    num_samples=10\n",
    "    alpha=20\n",
    "    K = beta_mean.shape[0] \n",
    "    A = x.shape[0]  \n",
    "    \n",
    "    sampled_rhos = jax.random.multivariate_normal(key, beta_mean, beta_cov, (num_samples,))    \n",
    "    scores = np.dot(sampled_rhos, x.T)\n",
    "    best_actions = np.argmax(scores, axis=1)\n",
    "    freq=np.zeros(A)\n",
    "    for action in range(A):\n",
    "        freq=freq.at[action].set(np.sum(best_actions == action) / num_samples)\n",
    "        \n",
    "    return np.log(freq[a])\n",
    "\n",
    "def igw_pi(beta_mean, x, beta_cov, a):\n",
    "    alpha=20\n",
    "    erewards = np.einsum('ij,j->i', x, beta_mean)  # prediction\n",
    "    best_arm = np.argmax(erewards)\n",
    "    gaps = erewards[best_arm] - erewards  # Gaps\n",
    "\n",
    "    A = x.shape[0]  # x is (A, K)\n",
    "    # Compute the prob for non-best \n",
    "    pi = 1 / (A + alpha * gaps)\n",
    "    pi.at[best_arm].set(0)  # temp\n",
    "\n",
    "    # Adjust the best arm\n",
    "    pi_best = 1 - np.sum(pi)\n",
    "    pi=pi.at[best_arm].set(pi_best)\n",
    "    \n",
    "    return np.log(pi[a])\n",
    "\n",
    "def _sample_rs_softmax(rhox, x, a, t, key, beta0_N, beta0_y, _betas_N, rs): #beta_mean, beta_cov, rhox, x, a, t, key, \n",
    "    keys = jax.random.split(key, 3)\n",
    "    ereward = np.dot(rhox, x[a])\n",
    "    \n",
    "    r = jax.random.normal(keys[0]) * sigma + ereward\n",
    "    _r = jax.random.normal(keys[1]) * sigma + ereward\n",
    "\n",
    "    # Update rs with the new sampled rewards for comparison\n",
    "    _rs = rs.at[t].set(r)\n",
    "    _rs_ = rs.at[t].set(_r)\n",
    "    \n",
    "    # Calculate updated beliefs for both reward samples\n",
    "    betas_mean, betas_cov = f_update(beta0_N, _betas_N, beta0_y, _rs)\n",
    "    betas_mean_, betas_cov_ = f_update(beta0_N, _betas_N, beta0_y, _rs_)\n",
    "    \n",
    "    # Compute softmax policy probabilities for both sets of beliefs\n",
    "    like = softmax_pi(betas_mean[t, :], x, betas_cov[t, :, :], a)\n",
    "    _like = softmax_pi(betas_mean_[t, :], x, betas_cov_[t, :, :], a)\n",
    "    \n",
    "    # Metropolis-Hastings condition to select reward\n",
    "    cond = _like - like > np.log(jax.random.uniform(keys[2]))\n",
    "    selected_r = jax.lax.select(cond, _r, r)\n",
    "    \n",
    "    # Return the selected reward and updated beliefs for the selected reward\n",
    "    updated_beliefs = f_update(beta0_N, _betas_N, beta0_y, rs.at[t].set(selected_r))\n",
    "    return selected_r, updated_beliefs\n",
    "\n",
    "def _sample_rs_igw(rhox, x, a, t, key, beta0_N, beta0_y, _betas_N, rs): #beta_mean, beta_cov, rhox, x, a, t, key, \n",
    "    keys = jax.random.split(key, 3)\n",
    "    ereward = np.dot(rhox, x[a])\n",
    "    \n",
    "    r = jax.random.normal(keys[0]) * sigma + ereward\n",
    "    _r = jax.random.normal(keys[1]) * sigma + ereward\n",
    "\n",
    "    # Update rs with the new sampled rewards for comparison\n",
    "    _rs = rs.at[t].set(r)\n",
    "    _rs_ = rs.at[t].set(_r)\n",
    "    \n",
    "    # Calculate updated beliefs for both reward samples\n",
    "    betas_mean, betas_cov = f_update(beta0_N, _betas_N, beta0_y, _rs)\n",
    "    betas_mean_, betas_cov_ = f_update(beta0_N, _betas_N, beta0_y, _rs_)\n",
    "    \n",
    "    # Compute softmax policy probabilities for both sets of beliefs\n",
    "    like = igw_pi(betas_mean[t, :], x, betas_cov[t, :, :], a)\n",
    "    _like = igw_pi(betas_mean_[t, :], x, betas_cov_[t, :, :], a)\n",
    "    \n",
    "    # Metropolis-Hastings condition to select reward\n",
    "    cond = _like - like > np.log(jax.random.uniform(keys[2]))\n",
    "    selected_r = jax.lax.select(cond, _r, r)\n",
    "    \n",
    "    # Return the selected reward and updated beliefs for the selected reward\n",
    "    updated_beliefs = f_update(beta0_N, _betas_N, beta0_y, rs.at[t].set(selected_r))\n",
    "    return selected_r, updated_beliefs\n",
    "\n",
    "def _sample_rs_ts(rhox, x, a, t, key, beta0_N, beta0_y, _betas_N, rs): #beta_mean, beta_cov, rhox, x, a, t, key, \n",
    "    keys = jax.random.split(key, 3)\n",
    "    ereward = np.dot(rhox, x[a])\n",
    "    \n",
    "    r = jax.random.normal(keys[0]) * sigma + ereward\n",
    "    _r = jax.random.normal(keys[1]) * sigma + ereward\n",
    "\n",
    "    # Update rs with the new sampled rewards for comparison\n",
    "    _rs = rs.at[t].set(r)\n",
    "    _rs_ = rs.at[t].set(_r)\n",
    "    \n",
    "    # Calculate updated beliefs for both reward samples\n",
    "    betas_mean, betas_cov = f_update(beta0_N, _betas_N, beta0_y, _rs)\n",
    "    betas_mean_, betas_cov_ = f_update(beta0_N, _betas_N, beta0_y, _rs_)\n",
    "    \n",
    "    # Compute softmax policy probabilities for both sets of beliefs\n",
    "    like = ts_pi(betas_mean[t, :], x, betas_cov[t, :, :], a,  keys[0])\n",
    "    _like = ts_pi(betas_mean_[t, :], x, betas_cov_[t, :, :], a,  keys[0])\n",
    "    \n",
    "    # Metropolis-Hastings condition to select reward\n",
    "    cond = _like - like > np.log(jax.random.uniform(keys[2]))\n",
    "    selected_r = jax.lax.select(cond, _r, r)\n",
    "    \n",
    "    # Return the selected reward and updated beliefs for the selected reward\n",
    "    updated_beliefs = f_update(beta0_N, _betas_N, beta0_y, rs.at[t].set(selected_r))\n",
    "    return selected_r, updated_beliefs\n",
    "\n",
    "\n",
    "def _sample_rs_optimistic(rhox, x, a, t, key, beta0_N, beta0_y, _betas_N, rs): #beta_mean, beta_cov, rhox, x, a, t, key, \n",
    "    keys = jax.random.split(key, 3)\n",
    "    ereward = np.dot(rhox, x[a])\n",
    "    \n",
    "    r = jax.random.normal(keys[0]) * sigma + ereward\n",
    "    _r = jax.random.normal(keys[1]) * sigma + ereward\n",
    "\n",
    "    # Update rs with the new sampled rewards for comparison\n",
    "    _rs = rs.at[t].set(r)\n",
    "    _rs_ = rs.at[t].set(_r)\n",
    "    \n",
    "    # Calculate updated beliefs for both reward samples\n",
    "    betas_mean, betas_cov = f_update(beta0_N, _betas_N, beta0_y, _rs)\n",
    "    betas_mean_, betas_cov_ = f_update(beta0_N, _betas_N, beta0_y, _rs_)\n",
    "    \n",
    "    # Compute softmax policy probabilities for both sets of beliefs\n",
    "    like = optimistic_pi(betas_mean[t, :], x, betas_cov[t, :, :], a)\n",
    "    _like = optimistic_pi(betas_mean_[t, :], x, betas_cov_[t, :, :], a)\n",
    "    \n",
    "    # Metropolis-Hastings condition to select reward\n",
    "    cond = _like - like > np.log(jax.random.uniform(keys[2]))\n",
    "    selected_r = jax.lax.select(cond, _r, r)\n",
    "    \n",
    "    # Return the selected reward and updated beliefs for the selected reward\n",
    "    updated_beliefs = f_update(beta0_N, _betas_N, beta0_y, rs.at[t].set(selected_r))\n",
    "    return selected_r, updated_beliefs\n",
    "\n",
    "# @jax.jit\n",
    "def sample_rs_softmax(args, keys):\n",
    "    (rhox, data_x, data_a, beta0_N, beta0_y, rs) = args\n",
    "    rs_init =rs\n",
    "    T, _, _ = data_x.shape\n",
    "    updated_rs = np.zeros(T)\n",
    "    updated_beliefs = None\n",
    "    \n",
    "    iter_samples = keys.shape[0]    \n",
    "    RS = np.zeros((iter_samples, T))\n",
    "\n",
    "    for iter_idx in range(iter_samples):\n",
    "        rs = np.copy(rs_init)  # Start with the initial set of rewards for each sample\\\n",
    "        key_sample =keys[iter_idx]        \n",
    "\n",
    "        for t in range(T):\n",
    "            x_t = data_x[t]\n",
    "            a_t = data_a[t]\n",
    "            key_timestep, key_sample = jax.random.split(key_sample)\n",
    "\n",
    "            # Sample rewards and update beliefs for the current timestep\n",
    "            sampled_r, beliefs_for_timestep = _sample_rs_softmax(rhox, x_t, a_t, t, key_timestep, beta0_N, beta0_y, _betas_N, rs)\n",
    "            rs=rs.at[t].set(sampled_r)\n",
    "            updated_beliefs = beliefs_for_timestep\n",
    "            \n",
    "            RS=RS.at[iter_idx, t].set(rs[t])  # Update the corresponding entry in RS\n",
    "\n",
    "    return RS\n",
    "\n",
    "\n",
    "# @jax.jit\n",
    "def sample_rs_optimistic(args, keys):\n",
    "    (rhox, data_x, data_a, beta0_N, beta0_y, rs) = args\n",
    "    rs_init =rs\n",
    "    T, _, _ = data_x.shape\n",
    "    updated_rs = np.zeros(T)\n",
    "    updated_beliefs = None\n",
    "    \n",
    "    iter_samples = keys.shape[0]    \n",
    "    RS = np.zeros((iter_samples, T))\n",
    "\n",
    "    for iter_idx in range(iter_samples):\n",
    "        rs = np.copy(rs_init)  # Start with the initial set of rewards for each sample\\\n",
    "        key_sample =keys[iter_idx]        \n",
    "\n",
    "        for t in range(T):\n",
    "            x_t = data_x[t]\n",
    "            a_t = data_a[t]\n",
    "            key_timestep, key_sample = jax.random.split(key_sample)\n",
    "\n",
    "            # Sample rewards and update beliefs for the current timestep\n",
    "            sampled_r, beliefs_for_timestep = _sample_rs_optimistics(rhox, x_t, a_t, t, key_timestep, beta0_N, beta0_y, _betas_N, rs)\n",
    "            rs=rs.at[t].set(sampled_r)\n",
    "            updated_beliefs = beliefs_for_timestep\n",
    "            \n",
    "            RS=RS.at[iter_idx, t].set(rs[t])  # Update the corresponding entry in RS\n",
    "\n",
    "    return RS\n",
    "\n",
    "\n",
    "# @jax.jit\n",
    "def sample_rs_igw(args, keys):\n",
    "    (rhox, data_x, data_a, beta0_N, beta0_y, rs) = args\n",
    "    rs_init =rs\n",
    "    T, _, _ = data_x.shape\n",
    "    updated_rs = np.zeros(T)\n",
    "    updated_beliefs = None\n",
    "    \n",
    "    iter_samples = keys.shape[0]    \n",
    "    RS = np.zeros((iter_samples, T))\n",
    "\n",
    "\n",
    "    for iter_idx in range(iter_samples):\n",
    "        rs = np.copy(rs_init)  # Start with the initial set of rewards for each sample\\\n",
    "        key_sample =keys[iter_idx]        \n",
    "\n",
    "        for t in range(T):\n",
    "            x_t = data_x[t]\n",
    "            a_t = data_a[t]\n",
    "            key_timestep, key_sample = jax.random.split(key_sample)\n",
    "\n",
    "            # Sample rewards and update beliefs for the current timestep\n",
    "            sampled_r, beliefs_for_timestep = _sample_rs_igw(rhox, x_t, a_t, t, key_timestep, beta0_N, beta0_y, _betas_N, rs)\n",
    "            rs=rs.at[t].set(sampled_r)\n",
    "            updated_beliefs = beliefs_for_timestep\n",
    "            \n",
    "            RS=RS.at[iter_idx, t].set(rs[t])  # Update the corresponding entry in RS\n",
    "\n",
    "    return RS\n",
    "\n",
    "# @jax.jit\n",
    "def sample_rs_ts(args, keys):\n",
    "    (rhox, data_x, data_a, beta0_N, beta0_y, rs) = args\n",
    "    rs_init =rs\n",
    "    T, _, _ = data_x.shape\n",
    "    updated_rs = np.zeros(T)\n",
    "    updated_beliefs = None\n",
    "    \n",
    "    iter_samples = keys.shape[0]    \n",
    "    RS = np.zeros((iter_samples, T))\n",
    "    \n",
    "    for iter_idx in range(iter_samples):\n",
    "        rs = np.copy(rs_init)  # Start with the initial set of rewards for each sample\\\n",
    "        key_sample =keys[iter_idx]        \n",
    "\n",
    "        for t in range(T):\n",
    "            x_t = data_x[t]\n",
    "            a_t = data_a[t]\n",
    "            key_timestep, key_sample = jax.random.split(key_sample)\n",
    "\n",
    "            # Sample rewards and update beliefs for the current timestep\n",
    "            sampled_r, beliefs_for_timestep = _sample_rs_ts(rhox, x_t, a_t, t, key_timestep, beta0_N, beta0_y, _betas_N, rs)\n",
    "            rs=rs.at[t].set(sampled_r)\n",
    "            updated_beliefs = beliefs_for_timestep\n",
    "            \n",
    "            RS=RS.at[iter_idx, t].set(rs[t])  # Update the corresponding entry in RS\n",
    "\n",
    "    return RS\n",
    "\n",
    "def f_update(beta0_N, _betas_N, beta0_y, rs):\n",
    "    betas_y_updated = _betas_y(rs[:-1])  \n",
    "    betas_invN = np.linalg.inv(beta0_N + _betas_N)  \n",
    "    \n",
    "    betas_mean = np.einsum('ijk,ik->ij', betas_invN, beta0_y + betas_y_updated)\n",
    "    betas_cov = betas_invN * sigma**2\n",
    "    \n",
    "    return betas_mean, betas_cov\n",
    "\n",
    "@jax.jit\n",
    "def compute_rhox(RS):\n",
    "    _beta_y = _BETAS_Y(RS[:, :-1])[:,-1,:].mean(axis=0)\n",
    "    _beta_N = _betas_N[-1]\n",
    "    rhox = np.einsum('ij,j->i', np.linalg.inv(_beta_N), _beta_y)\n",
    "    return rhox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38255f2-d379-49a6-aec8-b41b3c117851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_rs_igw(beta0_N, beta0_y, data_x, data_a, rs):\n",
    "    # Assuming beta0_N, beta0_y are your model parameters for this example\n",
    "    # Compute betas_mean, betas_cov using f_update or similar for the entire sequence\n",
    "    \n",
    "    \n",
    "    # Initialize likelihood sum\n",
    "    likelihood_sum = 0.0\n",
    "    for sample_rs in rs:\n",
    "        betas_mean, betas_cov = f_update(beta0_N, _betas_N, beta0_y, sample_rs)    \n",
    "        # Iterate over all timesteps\n",
    "        for t in range(T):\n",
    "            policy_output = igw_pi(betas_mean[t], data_x[t], betas_cov, data_a[t])                    \n",
    "            likelihood_sum += policy_output\n",
    "    \n",
    "    return (likelihood_sum/rs.shape[0]).mean()\n",
    "\n",
    "grad_likelihood_igw = jax.grad(likelihood_rs_igw)\n",
    "grad_likelihood_igw = jax.jit(grad_likelihood_igw)\n",
    "\n",
    "def likelihood_rs_ts(beta0_N, beta0_y, data_x, data_a, rs):    \n",
    "    betas_mean, betas_cov = f_update(beta0_N, _betas_N, beta0_y, rs)\n",
    "    \n",
    "   # Initialize likelihood sum\n",
    "    likelihood_sum = 0.0\n",
    "    for sample_rs in rs:\n",
    "        betas_mean, betas_cov = f_update(beta0_N, _betas_N, beta0_y, sample_rs)\n",
    "    \n",
    "        # Iterate over all timesteps\n",
    "        for t in range(T):        \n",
    "            policy_output = ts_pi(betas_mean[t], data_x[t], betas_cov, data_a[t])                \n",
    "            likelihood_sum += policy_output\n",
    "    \n",
    "    return (likelihood_sum/rs.shape[0]).mean()\n",
    "\n",
    "\n",
    "grad_likelihood_ts = jax.grad(likelihood_rs_ts)\n",
    "grad_likelihood_ts = jax.jit(grad_likelihood_ts)\n",
    "\n",
    "def likelihood_rs_optimistic(beta0_N, beta0_y, data_x, data_a, rs):\n",
    "    # Assuming beta0_N, beta0_y are your model parameters for this example\n",
    "    # Compute betas_mean, betas_cov using f_update or similar for the entire sequence\n",
    "    betas_mean, betas_cov = f_update(beta0_N, _betas_N, beta0_y, rs)\n",
    "    \n",
    "   # Initialize likelihood sum\n",
    "    likelihood_sum = 0.0\n",
    "    for sample_rs in rs:\n",
    "        betas_mean, betas_cov = f_update(beta0_N, _betas_N, beta0_y, sample_rs)\n",
    "    \n",
    "        # Iterate over all timesteps\n",
    "        for t in range(T):\n",
    "            policy_output = optimistic_pi(betas_mean[t], data_x[t], betas_cov, data_a[t])\n",
    "            likelihood_sum += policy_output\n",
    "    \n",
    "    return (likelihood_sum/rs.shape[0]).mean()\n",
    "\n",
    "\n",
    "grad_likelihood_optimistic = jax.grad(likelihood_rs_optimistic)\n",
    "grad_likelihood_optimistic = jax.jit(grad_likelihood_optimistic)\n",
    "\n",
    "\n",
    "def likelihood_rs_softmax(beta0_N, beta0_y, data_x, data_a, rs):\n",
    "\n",
    "    likelihood_sum = 0.0\n",
    "    for sample_rs in rs:\n",
    "        betas_mean, betas_cov = f_update(beta0_N, _betas_N, beta0_y, sample_rs)\n",
    "    \n",
    "        for t in range(T):\n",
    "            policy_output = softmax_pi(betas_mean[t], data_x[t], betas_cov, data_a[t])\n",
    "            likelihood_sum += policy_output\n",
    "    \n",
    "    return (likelihood_sum/rs.shape[0]).mean()\n",
    "\n",
    "\n",
    "grad_likelihood_softmax = jax.grad(likelihood_rs_softmax)\n",
    "grad_likelihood_softmax = jax.jit(grad_likelihood_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba6085a-907d-423c-993a-aa91d0754f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood_sum = 0.0\n",
    "for sample_rs in RS:\n",
    "    betas_mean, betas_cov = f_update(beta0_N, _betas_N, beta0_y, sample_rs)\n",
    "    \n",
    "    for t in range(T):\n",
    "        policy_output = softmax_pi(betas_mean[t], data_x[t], betas_cov, data_a[t])\n",
    "        likelihood_sum += policy_output\n",
    "    \n",
    "(likelihood_sum/rs.shape[0]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b6f5c0-458e-481c-abaa-412850a32a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_likelihood_softmax(beta0_N, beta0_y, data_x, data_a, RS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14922bb-05ff-4caa-990c-84e8da6f448d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"../Datasets\"\n",
    "input_dir = \"../Results\"\n",
    "\n",
    "RUNS=10\n",
    "import tqdm\n",
    "algos = [\"ts\", \"optimistic\", \"softmax\", \"igw\", \"ucb\", \"greedy\"]\n",
    "policies = [\"softmax\",  \"optimistic\", \"ts\",\"igw\"]\n",
    "policy_map = {\"optimistic\": 0, \"softmax\": 1, \"ts\": 2,  \"igw\": 3}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebb6fcd-36e3-4b8d-adea-8cddcab5bcb9",
   "metadata": {},
   "source": [
    "# Model ICB run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caa254c-90e4-4e6b-a8a8-f2eabf3d30d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "alpha = 20 # exploration parameter in the policy definition\n",
    "sigma = .10 # variance of the rewards\n",
    "\n",
    "for i in tqdm.tqdm(range(RUNS)):  \n",
    "    for algo in algos:\n",
    "        for policy in policies:\n",
    "            print(i, algo, policy)\n",
    "            policy_index = policy_map[policy]\n",
    "            filename = f'{output_dir}/dataset_{i}_{algo}.dill'\n",
    "            with open(filename, 'rb') as f:\n",
    "                data = dill.load(f)\n",
    "            \n",
    "                data_x = np.array(data['x'])  \n",
    "                data_a = np.array(data['a'])  \n",
    "                T,A,K = data_x.shape     \n",
    "\n",
    "                __betas_N = lambda t: np.einsum('i,j->ij', data_x[t,data_a[t]], data_x[t,data_a[t]])\n",
    "                __betas_N = jax.vmap(__betas_N)\n",
    "                _betas_N = __betas_N(np.arange(T-1)).cumsum(axis=0)\n",
    "                _betas_N = np.concatenate((np.zeros((K,K))[None,...], _betas_N))\n",
    "\n",
    "                __betas_y = lambda r, t: r * data_x[t,data_a[t]]\n",
    "                __betas_y = jax.vmap(__betas_y)\n",
    "\n",
    "                _betas_y = lambda rs: np.concatenate((np.zeros(K)[None,...], __betas_y(rs, np.arange(T-1)).cumsum(axis=0)))\n",
    "                _betas_y = jax.jit(_betas_y)\n",
    "                _BETAS_Y = jax.jit(jax.vmap(_betas_y))\n",
    "\n",
    "                # A dictionary of hyperparameters of the simulation\n",
    "                hyper = dict()\n",
    "                hyper['n_samples'] = 2\n",
    "                hyper['iter'] = 2\n",
    "\n",
    "                # initialize beta, rewards and rho_env (rhox)\n",
    "                params = {'beta0': 10e-4}\n",
    "                grad_mnsq = {'beta0': 10e-4}\n",
    "                key = jax.random.PRNGKey(0)\n",
    "                beta0_y, beta0_N = decode(params)\n",
    "                rs = jax.random.normal(key, shape=(T,))\n",
    "                rhox = -np.ones(K)/K\n",
    "\n",
    "                for j in range(hyper['iter']): \n",
    "                    key, subkey = jax.random.split(key)\n",
    "    \n",
    "                    # sample rewards\n",
    "                    if policy_index == 0:\n",
    "                        RS = sample_rs_optimistic((rhox, data_x, data_a, beta0_N, beta0_y, rs),  jax.random.split(subkey, hyper['n_samples'])  )\n",
    "                    elif policy_index == 1:\n",
    "                        RS = sample_rs_softmax((rhox, data_x, data_a, beta0_N, beta0_y, rs),  jax.random.split(subkey, hyper['n_samples'])  )\n",
    "                    elif policy_index == 2:\n",
    "                        RS = sample_rs_ts((rhox, data_x, data_a, beta0_N, beta0_y, rs),  jax.random.split(subkey, hyper['n_samples'])  )\n",
    "                    elif policy_index == 3:\n",
    "                        RS = sample_rs_igw((rhox, data_x, data_a, beta0_N, beta0_y, rs),  jax.random.split(subkey, hyper['n_samples'])  )\n",
    "\n",
    "                        \n",
    "                    rs = RS.mean(axis=0)\n",
    "                    rhox = compute_rhox(RS)\n",
    "                    \n",
    "                    # gradients\n",
    "                    if policy_index == 0:\n",
    "                        grad = grad_likelihood_optimistic(beta0_N, beta0_y, data_x, data_a, RS)\n",
    "                    elif policy_index == 1:\n",
    "                        grad = grad_likelihood_softmax(beta0_N, beta0_y, data_x, data_a, RS)\n",
    "                    elif policy_index == 2:\n",
    "                        grad = grad_likelihood_ts(beta0_N, beta0_y, data_x, data_a, RS)\n",
    "                    elif policy_index == 3:\n",
    "                        grad = grad_likelihood_igw(beta0_N, beta0_y, data_x, data_a, RS)\n",
    "                    \n",
    "                    grad_mnsq['beta0'] = .1 * grad['beta0']**2 + .9 * grad_mnsq['beta0']\n",
    "                    params['beta0'] += .001 * grad['beta0'] / (np.sqrt(grad_mnsq['beta0']) + 1e-8)\n",
    "                    beta0_y, beta0_N = decode(params)\n",
    "\n",
    "                rhox = rhox / np.abs(rhox).sum()\n",
    "    \n",
    "                res = dict()\n",
    "                res['rhox'] = rhox\n",
    "                res['beta0_y'] = beta0_y\n",
    "                res['beta0_N'] = beta0_N\n",
    "\n",
    "                key, subkey = jax.random.split(key)\n",
    "                RS = sample_rs((rs, rhox, data_x, data_a, beta0_N, beta0_y), jax.random.split(subkey, hyper['n_samples']))\n",
    "                BETAS_Y = beta0_y + _BETAS_Y #(RS[:, :-1])\n",
    "                betas_invN = np.linalg.inv(beta0_N + _betas_N)\n",
    "                betas_mean = np.einsum('ijk,lik->lij', betas_invN, BETAS_Y).mean(axis=0)\n",
    "                betas_cov = betas_invN * sigma**2\n",
    "\n",
    "                res['betas_mean'] = betas_mean\n",
    "                res['betas_cov'] = betas_invN\n",
    "                \n",
    "                print(betas_mean)\n",
    "                \n",
    "                # Save the results\n",
    "                filename = f'{input_dir}/dataset_{i}_{algo}_{policy}_MODEL.dill'\n",
    "                with open(filename, 'wb') as f:\n",
    "                    dill.dump(res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42ebc7b-8735-437c-848d-581493c7431a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
