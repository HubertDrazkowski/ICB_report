{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f323df3-05fb-41b3-b5c0-15ccca233192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import dill\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "from jax.scipy.special import logsumexp\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c6f40b-af01-46c0-8a9b-a19b9fa44b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNS=10\n",
    "input_dir = \"C:/Users/huber/Dropbox/ADULTERY/PHD/Cambridge_task/Datasets\"\n",
    "algos = [\"igw\", \"optimistic\", \"softmax\", \"greedy\", \"ucb\", \"ts\"]\n",
    "\n",
    "filename = f'{input_dir}/dataset_1_ucb.dill'\n",
    "with open(filename, 'rb') as f:\n",
    "    data = dill.load(f)\n",
    "            \n",
    "    data_x = np.array(data['x'])  \n",
    "    data_a = np.array(data['a'])  \n",
    "#    rhox = data['rhox']           \n",
    "#    betas_mean = data['betas_mean']\n",
    "#    betas_cov=data['betas_cov']\n",
    "    T,A,K = data_x.shape    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17717e22-7102-45ed-881e-954e62d5982f",
   "metadata": {},
   "source": [
    "# BICB UTILITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1e36e6-edf0-4f0c-8bba-65fa7f9771a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xs ~ constructs a matrix where each row represents the context vector selected by the action at each time step, \n",
    "# padded with zeros for time steps beyond the current one, this is reflected with t0, t1\n",
    "_xs = lambda t0, t1: jax.lax.select(t1 <= t0, data_x[t1,data_a[t1]], np.zeros(K)) \n",
    "_xs = jax.vmap(jax.vmap(_xs, in_axes=(None,0)), in_axes=(0,None)) \n",
    "xs = _xs(np.arange(T-1), np.arange(T-1)) \n",
    "\n",
    "# Belief calculation utilities\n",
    "\n",
    "## cumulative sum of outer products of contexts selected by actions up to timse t\n",
    "## later used for covariance matrix updates\n",
    "__betas_N = lambda t: np.einsum('i,j->ij', data_x[t,data_a[t]], data_x[t,data_a[t]])\n",
    "__betas_N = jax.vmap(__betas_N)\n",
    "_betas_N = __betas_N(np.arange(T-1)).cumsum(axis=0)\n",
    "_betas_N = np.concatenate((np.zeros((K,K))[None,...], _betas_N))\n",
    "\n",
    "\n",
    "## cumulative sum of dot product of rewards and context for each time step\n",
    "## later used for posterior mean updates\n",
    "__betas_y = lambda r, t: r * data_x[t,data_a[t]]\n",
    "__betas_y = jax.vmap(__betas_y)\n",
    "_betas_y = lambda rs: np.concatenate((np.zeros(K)[None,...], __betas_y(rs, np.arange(T-1)).cumsum(axis=0)))\n",
    "_betas_y = jax.jit(_betas_y)\n",
    "_BETAS_Y = jax.jit(jax.vmap(_betas_y))\n",
    "\n",
    "@jax.jit\n",
    "def decode(params):\n",
    "    '''\n",
    "    initialize parameters \n",
    "    '''\n",
    "    beta0 = np.exp(20 * params['beta0']) # WHY 20 HERE for? (it is 1 in the end)\n",
    "    beta0_y = -np.ones(K)/K * beta0  # vector\n",
    "    beta0_N = np.eye(K) * beta0      # matrix\n",
    "    return beta0_y, beta0_N\n",
    "\n",
    "def _sample_rhos_like(rho, x, a):\n",
    "    '''\n",
    "    calculate log likelihood scores for each action using softmax   \n",
    "    '''\n",
    "    q = alpha * np.einsum('ij,j->i', x, rho) # dot product <context vector, parameter> for each action\n",
    "    return q[a] - logsumexp(q) #LogSumExp - after exponentiation sum to 1? log-probability of choosing action over the other\n",
    "\n",
    "def _sample_rhos(beta_mean, beta_cov, x, a, key):\n",
    "    '''\n",
    "    Sample two rhos, do a round of Metropolis Hastings\n",
    "    '''\n",
    "    keys = jax.random.split(key, 3) # pseud random seed controlling\n",
    "    \n",
    "    rho  = jax.random.multivariate_normal(keys[0], beta_mean, beta_cov) # sample rho'\n",
    "    _rho = jax.random.multivariate_normal(keys[1], beta_mean, beta_cov) # sample rho''\n",
    "    \n",
    "    like  = _sample_rhos_like(rho, x, a) # calc like scores rho' \n",
    "    _like = _sample_rhos_like(_rho, x, a) # calc like scores rho''\n",
    "    \n",
    "    cond  = _like - like > np.log(jax.random.uniform(keys[2])) # Metropolis Hastings\n",
    "    return jax.lax.select(cond, _rho, rho) # _rho if cond true rho otherwise\n",
    "\n",
    "_sample_rhos = jax.vmap(_sample_rhos)\n",
    "\n",
    "def _sample_rs_init(rhox, key):\n",
    "    '''\n",
    "    expected reward for each action at the last time step T-1 + add gaussian noise and return\n",
    "    '''\n",
    "    mean = np.einsum('ij,j->i', xs[-1], rhox)\n",
    "    rs = mean + sigma * jax.random.normal(key, shape=(T-1,))\n",
    "    return rs\n",
    "\n",
    "def _sample_rs(rhox, rhos, beta0_y, betas_invN, key):\n",
    "    '''\n",
    "    Lemma 1 sampling\n",
    "    '''\n",
    "    invcov = np.eye(T-1) # initialize for inverse covariance 1/σ2 I\n",
    "        # outer product of xs, weighted by inverse covariance of believes?\n",
    "        # sum over all time steps 'a' \n",
    "        # cross-product 'bc' and 'de' indices\n",
    "        # inverse covariance weights `cd' indices\n",
    "        # \\sum^T_t=2 X^T_{t−1} Σ_t X_{t−1}\n",
    "    invcov = invcov + np.einsum('abc,acd,aed->be', xs, betas_invN[1:], xs) \n",
    "        # dot product of the last context vectors xs[-1] with the parameters rhox\n",
    "        # (X^T_t \\rhox)\n",
    "    invcov_at_mean = np.einsum('ij,j->i', xs[-1], rhox)\n",
    "        # 1) product of each context vector with its corresponding parameter estimate adjustment\n",
    "        # 2) weighted sum of these adjustments across all contexts xs and adjusted parameters rhos[1:] - ...\n",
    "        # \\sum^T_t=2 X^T_t-1(\\rho_t - C_t C^{-1}_1 \\mu_1)\n",
    "    invcov_at_mean = invcov_at_mean + np.einsum('ijk,ik->j', xs, rhos[1:] - np.einsum('ijk,k->ij', betas_invN[1:], beta0_y))\n",
    "        # \\tilde{C}\n",
    "    cov = np.linalg.inv(invcov)\n",
    "        # \\tilde{\\mu}\n",
    "    mean = cov @ invcov_at_mean\n",
    "    # sample normal\n",
    "    rs = jax.random.multivariate_normal(key, mean, cov * sigma**2) # sampling the reward from a multivariate normal\n",
    "    return rs\n",
    "\n",
    "def _sample(arg0, arg1):\n",
    "    '''\n",
    "    Updating the posterior mean and covariance. \n",
    "    Sampling new env states (rhos) and rewards (rs) \n",
    "        based on the updated beliefs (betas_mean, betas_cov), using the context data (data_x, data_a)\n",
    "    '''\n",
    "    (rhos, rs, rhox, beta0_y, betas_invN), key = arg0, arg1 # unpack\n",
    "    keys = jax.random.split(key, 2) # pseudo random seed\n",
    "    \n",
    "    # calculate beta mean and cov \n",
    "    # using cumulative dot product of rewards and context _betas_y(rs)) \n",
    "    betas_mean = np.einsum('ijk,ik->ij', betas_invN, beta0_y + _betas_y(rs)) \n",
    "    betas_cov = betas_invN * sigma**2 \n",
    "    \n",
    "    # sample rhos and rewards\n",
    "    rhos = _sample_rhos(betas_mean, betas_cov, data_x, data_a, jax.random.split(keys[0], T))\n",
    "    rs = _sample_rs(rhox, rhos, beta0_y, betas_invN, keys[1])\n",
    "    \n",
    "    return (rhos, rs, rhox, beta0_y, betas_invN), (rhos, rs)\n",
    "\n",
    "@jax.jit\n",
    "def sample(rhox, beta0_y, beta0_N, key):\n",
    "    '''\n",
    "    Orchestration of sampling that leads to sequences of rhos and rewards\n",
    "    '''\n",
    "    keys = jax.random.split(key, 2) # pseudorandom seed keys: 2\n",
    "    \n",
    "        # inverse of the sum of:\n",
    "        # (1) the initial belief \"beta0_N\"\n",
    "        # (2) cumulative outer product of contexts selected by actions \"_betas_N\"\n",
    "    betas_invN = np.linalg.inv(beta0_N + _betas_N) \n",
    "        # reward sampling based on current rho\n",
    "    rs = _sample_rs_init(rhox, keys[0]) #\n",
    "        # iteratively apply the \"_sample\" function across time steps, generating sequences of rewards and states\n",
    "    _, (_RHOS, _RS) = jax.lax.scan(_sample, (np.zeros((T,K)), rs, rhox, beta0_y, betas_invN), jax.random.split(keys[1], hyper['sample_stop']))\n",
    "         # initial samples were for burn-in purposes\n",
    "    RHOS = _RHOS[hyper['sample_start']::hyper['sample_step']]\n",
    "    RS = _RS[hyper['sample_start']::hyper['sample_step']]\n",
    "    \n",
    "    return RHOS, RS\n",
    "\n",
    "def compute_rhox(RS):\n",
    "    '''\n",
    "    the mean of the product of rewards and contexts over all sampled paths\n",
    "    '''\n",
    "    # updated estimate of the expected rewards conditioned on the actions taken\n",
    "    _beta_y = _BETAS_Y(RS)[:,-1,:].mean(axis=0)\n",
    "    # the last cumulative sum of outer products of contexts\n",
    "    _beta_N = _betas_N[-1]\n",
    "    # solving a linear system \n",
    "    # _beta_N coefficient matrix \n",
    "    # _beta_y vector term \n",
    "    rhox = np.einsum('ij,j->i', np.linalg.inv(_beta_N), _beta_y)\n",
    "    return rhox\n",
    "\n",
    "def _likelihood0(rho, beta_mean, beta_invcov):\n",
    "    '''\n",
    "    Q + logP: calculating log likelihood  | beta_mean beta_invcov\n",
    "    '''\n",
    "    # negative \n",
    "    res = -np.einsum('i,ij,j->', rho-beta_mean, beta_invcov, rho-beta_mean)\n",
    "    res = res + np.log(np.linalg.det(beta_invcov))\n",
    "    return res\n",
    "\n",
    "_likelihood0 = jax.vmap(_likelihood0)\n",
    "\n",
    "def _likelihood1(params, rhos, rs):\n",
    "    '''\n",
    "    Updating posterior beliefs, mean covariance then _likelihood0\n",
    "    '''\n",
    "    beta0_y, beta0_N = decode(params) # initialize parameters\n",
    "    betas_y = beta0_y + _betas_y(rs)\n",
    "    betas_N = beta0_N + _betas_N\n",
    "    betas_invN = np.linalg.inv(betas_N)\n",
    "    betas_mean = np.einsum('ijk,ik->ij', betas_invN, betas_y)\n",
    "    betas_invcov = betas_N / sigma**2\n",
    "    \n",
    "    return _likelihood0(rhos, betas_mean, betas_invcov).sum()\n",
    "\n",
    "_likelihood1 = jax.vmap(_likelihood1, in_axes=(None,0,0))\n",
    "\n",
    "def likelihood(params, RHOS, RS):\n",
    "    '''\n",
    "    Q mean\n",
    "    '''\n",
    "    return _likelihood1(params, RHOS, RS).mean()\n",
    "\n",
    "grad_likelihood = jax.grad(likelihood)\n",
    "grad_likelihood = jax.jit(grad_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76143123-d62a-44ca-967b-cba6fd7bb75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"../Datasets\"\n",
    "input_dir = \"../Results\"\n",
    "algos = [\"igw\", \"optimistic\", \"softmax\", \"greedy\", \"ucb\", \"ts\"]\n",
    "RUNS=10\n",
    "\n",
    "# A dictionary of hyperparameters of the simulation\n",
    "hyper = dict()\n",
    " \n",
    "############### ADJUST HYPER AND THE LOADING\n",
    "# hyper['sample_start'] = 500\n",
    "# hyper['sample_stop'] = 1_000\n",
    "# hyper['sample_step'] = 1\n",
    "# hyper['iter'] = 10\n",
    "\n",
    "# data_x = np.array(data['x'])  \n",
    "# data_a = np.array(data['a'])  \n",
    "# rhox = data['rhox']           \n",
    "# betas_mean = data['betas_mean']\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008b842a-aac9-434e-bb59-9b35ee0b15f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm.tqdm(range(RUNS)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9bfcda-2e2e-49ff-9c43-3e7275ffe26b",
   "metadata": {},
   "source": [
    "# BICB RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a402549-c5bd-44b6-94d3-1bc6522e908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "alpha = 20 # exploration parameter in the policy definition\n",
    "sigma = .10 # variance of the rewards\n",
    "\n",
    "for i in tqdm.tqdm(range(1,10)):  # Assuming 100 datasets\\\n",
    "    for algo in algos:\n",
    "        print(i, algo)\n",
    "        filename = f'{output_dir}/dataset_{i}_{algo}.dill'\n",
    "        with open(filename, 'rb') as f:\n",
    "            data = dill.load(f)\n",
    "            \n",
    "            data_x = np.array(data['x'])  \n",
    "            data_a = np.array(data['a'])  \n",
    "            rhox = data['rhox']           \n",
    "            betas_mean = data['betas_mean']\n",
    "            betas_cov=data['betas_cov']\n",
    "\n",
    "            T,A,K = data_x.shape    \n",
    "            \n",
    "            # A dictionary of hyperparameters of the simulation\n",
    "            hyper = dict()\n",
    "            hyper['sample_start'] = 1_000\n",
    "            hyper['sample_stop'] = 2_000\n",
    "            hyper['sample_step'] = 1\n",
    "            hyper['iter'] = 20\n",
    "\n",
    "            _xs = lambda t0, t1: jax.lax.select(t1 <= t0, data_x[t1,data_a[t1]], np.zeros(K)) \n",
    "            _xs = jax.vmap(jax.vmap(_xs, in_axes=(None,0)), in_axes=(0,None)) \n",
    "            xs = _xs(np.arange(T-1), np.arange(T-1)) \n",
    "    \n",
    "            __betas_N = lambda t: np.einsum('i,j->ij', data_x[t,data_a[t]], data_x[t,data_a[t]])\n",
    "            __betas_N = jax.vmap(__betas_N)\n",
    "            _betas_N = __betas_N(np.arange(T-1)).cumsum(axis=0)\n",
    "            _betas_N = np.concatenate((np.zeros((K,K))[None,...], _betas_N))\n",
    "\n",
    "            __betas_y = lambda r, t: r * data_x[t,data_a[t]]\n",
    "            __betas_y = jax.vmap(__betas_y)\n",
    "            _betas_y = lambda rs: np.concatenate((np.zeros(K)[None,...], __betas_y(rs, np.arange(T-1)).cumsum(axis=0)))\n",
    "            _betas_y = jax.jit(_betas_y)\n",
    "            _BETAS_Y = jax.jit(jax.vmap(_betas_y))\n",
    "            \n",
    "            rhox = -np.ones(K)/K\n",
    "            params = {'beta0': 0.}\n",
    "            grad_mnsq = {'beta0': 0.}\n",
    "            beta0_y, beta0_N = decode(params)\n",
    "\n",
    "            for j in range(hyper['iter']):  #tqdm.tqdm()\n",
    "    \n",
    "                key, subkey = jax.random.split(key)\n",
    "                RHOS, RS = sample(rhox, beta0_y, beta0_N, subkey)\n",
    "\n",
    "                rhox = compute_rhox(RS)\n",
    "\n",
    "                grad = grad_likelihood(params, RHOS, RS)\n",
    "                grad_mnsq['beta0'] = .1 * grad['beta0']**2 + .9 * grad_mnsq['beta0']\n",
    "                params['beta0'] += .001 * grad['beta0'] / (np.sqrt(grad_mnsq['beta0']) + 1e-8)\n",
    "                beta0_y, beta0_N = decode(params)\n",
    "\n",
    "            # print(rhox, beta0_N[0,0])\n",
    "\n",
    "            rhox = rhox / np.abs(rhox).sum()\n",
    "\n",
    "            res = dict()\n",
    "            res['rhox'] = rhox\n",
    "            res['beta0_y'] = beta0_y\n",
    "            res['beta0_N'] = beta0_N\n",
    "\n",
    "            key, subkey = jax.random.split(key)\n",
    "            _, RS = sample(rhox, beta0_y, beta0_N, subkey)\n",
    "\n",
    "            BETAS_Y = beta0_y + _BETAS_Y(RS)\n",
    "            betas_invN = np.linalg.inv(beta0_N + _betas_N)\n",
    "            betas_mean = np.einsum('ijk,lik->lij', betas_invN, BETAS_Y).mean(axis=0)\n",
    "            betas_cov = betas_invN * sigma**2\n",
    "\n",
    "            res['betas_mean'] = betas_mean\n",
    "            res['betas_cov'] = betas_invN\n",
    "\n",
    "            print(res['betas_mean'])\n",
    "            \n",
    "            # Save the results\n",
    "            filename = f'{input_dir}/dataset_{i}_{algo}_BICB.dill'\n",
    "            with open(filename, 'wb') as f:\n",
    "                dill.dump(res, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee8ed00-4e84-45a5-b4a5-4c2119ff53ad",
   "metadata": {},
   "source": [
    "# NBICB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ab3a1a-da8c-4cf4-be5b-fc52fd86423b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import dill\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "import numpy as np1\n",
    "from jax.scipy.special import logsumexp\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f57f0c8-c71d-4635-9d9c-7968e8da970d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#jax.config.update('jax_platform_name', 'cpu')\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('-i', '--input', required=True)\n",
    "# parser.add_argument('-o', '--output', default='res/general.obj')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "hyper = dict()\n",
    "hyper['sample_start'] = 10_000\n",
    "hyper['sample_stop'] = 20_000\n",
    "hyper['sample_step'] = 10\n",
    "hyper['variance_rho'] = 5e-4\n",
    "hyper['variance_beta'] = 5e-5\n",
    "hyper['offset'] = -np.ones(K)/K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8920ff-5f5e-4405-8648-e27bc7cf55e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_cov_T = lambda t0, t1: np.minimum(t0, t1) + 1\n",
    "_cov_T = jax.vmap(jax.vmap(_cov_T, in_axes=(None,0)), in_axes=(0,None))\n",
    "cov_T = _cov_T(np.arange(T), np.arange(T))\n",
    "\n",
    "_cov_K = np.eye(K)\n",
    "_cov_K = _cov_K.at[:,0].set(np.ones(K))\n",
    "_cov_K = _cov_K.at[:,0].set(_cov_K[:,0] / np.sum(_cov_K[:,0]**2)**.5)\n",
    "for i in range(1,K):\n",
    "    for j in range(i):\n",
    "        _cov_K = _cov_K.at[:,i].add(-np.sum(_cov_K[:,i] * _cov_K[:,j]) * _cov_K[:,j])\n",
    "    _cov_K = _cov_K.at[:,i].set(_cov_K[:,i] / np.sum(_cov_K[:,i]**2)**.5)\n",
    "_scale = np.eye(K)\n",
    "_scale = _scale.at[0,0].set(.1)\n",
    "_cov_K = _cov_K @ _scale @ np.linalg.inv(_cov_K)\n",
    "cov_K = _cov_K @ _cov_K.T\n",
    "\n",
    "cov_rho = cov_K * hyper['variance_rho']\n",
    "cov_rhos = np.kron(np.eye(T), cov_K) * hyper['variance_rho']\n",
    "cov_betas = np.kron(cov_T, cov_K) * hyper['variance_beta']\n",
    "invcov_rhos = np.linalg.inv(cov_rhos)\n",
    "invcov_betas = np.linalg.inv(cov_betas)\n",
    "mean_betas = hyper['offset'][None,...].repeat(T, axis=0).reshape(-1)\n",
    "cov = np.linalg.inv(invcov_rhos + invcov_betas)\n",
    "cov_at_invcov_betas_at_mean_betas = cov @ invcov_betas @ mean_betas\n",
    "cov_at_invcov_rhos = cov @ invcov_rhos\n",
    "\n",
    "cov_rho_L = np1.linalg.cholesky(cov_rho)\n",
    "cov_L = np1.linalg.cholesky(cov)\n",
    "\n",
    "def _sample_rhos_like(rho, x, a):\n",
    "    q = alpha * np.einsum('ij,j->i', x, rho)\n",
    "    return q[a] - logsumexp(q)\n",
    "\n",
    "def _sample_rhos(beta, x, a, key):\n",
    "    keys = jax.random.split(key, 3)\n",
    "    rho = beta + cov_rho_L @ jax.random.normal(keys[0], shape=(K,))\n",
    "    _rho = beta + cov_rho_L @ jax.random.normal(keys[1], shape=(K,))\n",
    "    like = _sample_rhos_like(rho, x, a)\n",
    "    _like = _sample_rhos_like(_rho, x, a)\n",
    "    cond = _like - like > np.log(jax.random.uniform(keys[2]))\n",
    "    return jax.lax.select(cond, _rho, rho)\n",
    "_sample_rhos = jax.vmap(_sample_rhos)\n",
    "\n",
    "def _sample_betas(rhos, key):\n",
    "    mean = cov_at_invcov_betas_at_mean_betas + cov_at_invcov_rhos @ rhos.reshape(-1)\n",
    "    _betas = mean + cov_L @ jax.random.normal(key, shape=(T*K,))\n",
    "    return _betas.reshape(-1,K)\n",
    "\n",
    "def _sample(arg0, arg1):\n",
    "    (rhos, betas), key = arg0, arg1\n",
    "    keys = jax.random.split(key, 2)\n",
    "    rhos = _sample_rhos(betas, data_x, data_a, jax.random.split(keys[0], T))\n",
    "    betas = _sample_betas(rhos, keys[1])\n",
    "    return (rhos, betas), (rhos, betas)\n",
    "\n",
    "def sample(rhos, betas, key, count):\n",
    "    (rhos, betas), (RHOS, BETAS) = jax.lax.scan(_sample, (rhos, betas), jax.random.split(key, count))\n",
    "    return rhos, betas, RHOS, BETAS\n",
    "sample = jax.jit(sample, static_argnums=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b7123f-b605-4389-84d3-342662f03c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm.tqdm(range(1,10)):  # Assuming 100 datasets\n",
    "    for algo in algos:\n",
    "        filename = f'{output_dir}/dataset_{i}_{algo}.dill'\n",
    "        with open(filename, 'rb') as f:\n",
    "            data = dill.load(f)\n",
    "            \n",
    "            data_x = np.array(data['x'])  \n",
    "            data_a = np.array(data['a'])  \n",
    "            rhox = data['rhox']           \n",
    "            betas_mean = data['betas_mean']\n",
    "            betas_cov=data['betas_cov']\n",
    "\n",
    "            T,A,K = data_x.shape    \n",
    "\n",
    "            rhos = np.zeros((T,K))\n",
    "            betas = np.zeros((T,K)) + hyper['offset']\n",
    "\n",
    "            BETAS = np.zeros((0,T,K))\n",
    "            for j in range(hyper['sample_stop'] // 200): #tqdm.tqdm(, unit_scale=200)\n",
    "                key, subkey = jax.random.split(key)\n",
    "                rhos, betas, _RHOS, _BETAS = sample(rhos, betas, subkey, 200)\n",
    "                BETAS = np.concatenate((BETAS, _BETAS))\n",
    "\n",
    "            betas = BETAS[hyper['sample_start']::hyper['sample_step']].mean(axis=0)\n",
    "            betas = betas / np.abs(betas).sum(axis=-1, keepdims=True)\n",
    "\n",
    "            res = dict()\n",
    "            res['betas'] = betas\n",
    "            \n",
    "            filename = f'{input_dir}/dataset_{i}_{algo}_NBICB.dill'\n",
    "            with open(filename, 'wb') as f:\n",
    "                dill.dump(res, f)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cf0f24-d933-4b0d-b0a4-c14cce182c77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
