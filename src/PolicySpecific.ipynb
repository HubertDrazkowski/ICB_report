{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import dill\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "from jax import lax\n",
    "from jax.scipy.special import logsumexp\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNS=1\n",
    "input_dir = \"../Datasets\"\n",
    "algos = [\"optimistic\", \"softmax\", \"igw\", \"greedy\", \"ucb\", \"ts\"]\n",
    "\n",
    "filename = f'{input_dir}/dataset_1_ucb.dill'\n",
    "with open(filename, 'rb') as f:\n",
    "    data = dill.load(f)\n",
    "            \n",
    "    data_x = np.array(data['x'])  \n",
    "    data_a = np.array(data['a'])  \n",
    "#    rhox = data['rhox']           \n",
    "#    betas_mean = data['betas_mean']\n",
    "#    betas_cov=data['betas_cov']\n",
    "    T,A,K = data_x.shape    \n",
    "\n",
    "# A dictionary of hyperparameters of the simulation\n",
    "hyper = dict()\n",
    " \n",
    "############### ADJUST HYPER AND THE LOADING\n",
    "hyper['sample_start'] = 1_000\n",
    "hyper['sample_stop'] = 2_000\n",
    "hyper['sample_step'] = 1\n",
    "hyper['iter'] = 100\n",
    "\n",
    "T = data_x.shape[0] # time dimension, total number of steps\n",
    "A = data_x.shape[1] # this is not used anywhere !!! Each action has it's own context, this is unusual, interesting setup\n",
    "K = data_x.shape[2] # feature space dimension, context shape\n",
    "alpha = 20 # exploration parameter in the policy definition\n",
    "sigma = .10 # variance of the rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC-BICB UTILITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## xs ~ constructs a matrix where each row represents the context vector selected by the action at each time step, \n",
    "## padded with zeros for time steps beyond the current one, this is reflected with t0, t1\n",
    "_xs = lambda t0, t1: jax.lax.select(t1 <= t0, data_x[t1,data_a[t1]], np.zeros(K)) \n",
    "_xs = jax.vmap(jax.vmap(_xs, in_axes=(None,0)), in_axes=(0,None)) \n",
    "xs = _xs(np.arange(T-1), np.arange(T-1)) \n",
    "\n",
    "\n",
    "# Belief updates utilities\n",
    "\n",
    "## cumulative sum of outer products of contexts selected by actions up to time t\n",
    "## later used for covariance matrix updates\n",
    "__betas_N = lambda t: np.einsum('i,j->ij', data_x[t,data_a[t]], data_x[t,data_a[t]])\n",
    "__betas_N = jax.vmap(__betas_N)\n",
    "_betas_N = __betas_N(np.arange(T-1)).cumsum(axis=0)\n",
    "_betas_N = np.concatenate((np.zeros((K,K))[None,...], _betas_N))\n",
    "\n",
    "\n",
    "## cumulative sum of dot product of rewards and context for each time step\n",
    "## later used for posterior mean updates\n",
    "__betas_y = lambda r, t: r * data_x[t,data_a[t]]\n",
    "__betas_y = jax.vmap(__betas_y)\n",
    "_betas_y = lambda rs: np.concatenate((np.zeros(K)[None,...], __betas_y(rs, np.arange(T-1)).cumsum(axis=0)))\n",
    "_betas_y = jax.jit(_betas_y)\n",
    "_BETAS_Y = jax.jit(jax.vmap(_betas_y))\n",
    "\n",
    "@jax.jit\n",
    "def decode(params):\n",
    "    '''\n",
    "    initialize parameters \n",
    "    '''\n",
    "    beta0 = np.exp(20 * params['beta0']) # WHY 20 HERE for? (it is 1 in the end)\n",
    "    beta0_y = -np.ones(K)/K * beta0  # vector\n",
    "    beta0_N = np.eye(K) * beta0      # matrix\n",
    "    return beta0_y, beta0_N\n",
    "\n",
    "\n",
    "# Implement likelihood functions for each policy\n",
    "def optimistic_like(rho, x, beta_cov, a):\n",
    "    alpha=20\n",
    "    q = alpha * np.einsum('ij,j->i', x, rho) + np.einsum('ij,jk,ki->i', x, beta_cov, x.T)\n",
    "    \n",
    "    return q[a] - logsumexp(q)    \n",
    "\n",
    "def softmax_like(rho, x,  beta_cov, a):\n",
    "    alpha=20\n",
    "    q = alpha * np.einsum('ij,j->i', x, rho)\n",
    "    \n",
    "    return q[a] - logsumexp(q) \n",
    "\n",
    "def ts_like(rho, x, beta_cov, a, key):\n",
    "    num_samples=10\n",
    "    alpha=20\n",
    "    K = rho.shape[0]  # Number of features\n",
    "    A = x.shape[0]  # Number of actions\n",
    "\n",
    "    # Result(num_samples, K) #sampled_rhos = np.random.multivariate_normal(rho, beta_cov, size=num_samples)\n",
    "    sampled_rhos = jax.random.multivariate_normal(key, rho, beta_cov, (num_samples,))\n",
    "        \n",
    "    # x to (1, A, K) and broadcasted \n",
    "    # Result shape (num_samples, A)\n",
    "    scores = np.dot(sampled_rhos, x.T)\n",
    "\n",
    "    # Result shape (num_samples,)\n",
    "    best_actions = np.argmax(scores, axis=1)\n",
    "    \n",
    "#    counts = np.bincount(best_actions, minlength=A)\n",
    "#    freq = best_action_counts / num_samples\n",
    "    freq=np.zeros(A)\n",
    "    for action in range(A):\n",
    "        freq=freq.at[action].set(np.sum(best_actions == action) / num_samples)\n",
    "        \n",
    "    return np.log(freq[a])\n",
    "    \n",
    "\n",
    "def igw_like(rho, x, beta_cov, a):\n",
    "    alpha=20\n",
    "    erewards = np.einsum('ij,j->i', x, rho)  # prediction\n",
    "    best_arm = np.argmax(erewards)\n",
    "    gaps = erewards[best_arm] - erewards  # Gaps\n",
    "\n",
    "    A = x.shape[0]  # x is (A, K)\n",
    "    # Compute the prob for non-best \n",
    "    pi = 1 / (A + alpha * gaps)\n",
    "    pi=pi.at[best_arm].set(0)  # temp\n",
    "\n",
    "    # Adjust the best arm\n",
    "    pi_best = 1 - np.sum(pi)\n",
    "    pi=pi.at[best_arm].set(pi_best)\n",
    "    \n",
    "    return np.log(pi[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sample_rhos_optimistic(beta_mean, beta_cov, x, a, key): #alpha,\n",
    "    '''\n",
    "    Sample two rhos, do a round of Metropolis Hastings\n",
    "    '''\n",
    "    keys = jax.random.split(key, 3) # pseud random seed controlling\n",
    "    \n",
    "    rho  = jax.random.multivariate_normal(keys[0], beta_mean, beta_cov) # sample rho'\n",
    "    _rho = jax.random.multivariate_normal(keys[1], beta_mean, beta_cov) # sample rho''\n",
    "    \n",
    "    like  = optimistic_like(rho, x, beta_cov, a) # calc like scores rho'  #, alpha\n",
    "    _like = optimistic_like(_rho, x, beta_cov, a) # calc like scores rho'' #, alpha\n",
    "    \n",
    "    cond  = _like - like > np.log(jax.random.uniform(keys[2])) # Metropolis Hastings\n",
    "    return jax.lax.select(cond, _rho, rho) # _rho if cond true rho otherwise\n",
    "\n",
    "_sample_rhos_optimistic = jax.vmap(_sample_rhos_optimistic)\n",
    "\n",
    "\n",
    "def _sample_rhos_softmax(beta_mean, beta_cov, x, a, key): #alpha,\n",
    "    '''\n",
    "    Sample two rhos, do a round of Metropolis Hastings\n",
    "    '''\n",
    "    keys = jax.random.split(key, 3) # pseud random seed controlling\n",
    "    \n",
    "    rho  = jax.random.multivariate_normal(keys[0], beta_mean, beta_cov) # sample rho'\n",
    "    _rho = jax.random.multivariate_normal(keys[1], beta_mean, beta_cov) # sample rho''\n",
    "    \n",
    "    like  = softmax_like(rho, x, beta_cov, a) # calc like scores rho'  #, alpha\n",
    "    _like = softmax_like(_rho, x, beta_cov, a) # calc like scores rho'' #, alpha\n",
    "    \n",
    "    cond  = _like - like > np.log(jax.random.uniform(keys[2])) # Metropolis Hastings\n",
    "    return jax.lax.select(cond, _rho, rho) # _rho if cond true rho otherwise\n",
    "\n",
    "_sample_rhos_softmax = jax.vmap(_sample_rhos_softmax)\n",
    "\n",
    "\n",
    "def _sample_rhos_ts(beta_mean, beta_cov, x, a, key): #alpha\n",
    "    '''\n",
    "    Sample two rhos, do a round of Metropolis Hastings\n",
    "    '''\n",
    "    keys = jax.random.split(key, 3) # pseud random seed controlling\n",
    "    \n",
    "    rho  = jax.random.multivariate_normal(keys[0], beta_mean, beta_cov) # sample rho'\n",
    "    _rho = jax.random.multivariate_normal(keys[1], beta_mean, beta_cov) # sample rho''\n",
    "    \n",
    "    like  = ts_like(rho, x, beta_cov, a, keys[0]) # calc like scores rho' #, alpha\n",
    "    _like = ts_like(_rho, x, beta_cov, a, keys[1]) # calc like scores rho'' #, alpha\n",
    "    \n",
    "    cond  = _like - like > np.log(jax.random.uniform(keys[2])) # Metropolis Hastings\n",
    "    return jax.lax.select(cond, _rho, rho) # _rho if cond true rho otherwise\n",
    "\n",
    "_sample_rhos_ts = jax.vmap(_sample_rhos_ts)\n",
    "\n",
    "\n",
    "def _sample_rhos_igw(beta_mean, beta_cov, x, a, key): #, alpha\n",
    "    '''\n",
    "    Sample two rhos, do a round of Metropolis Hastings\n",
    "    '''\n",
    "    print(key, key.shape)\n",
    "    keys = jax.random.split(key, 3) # pseud random seed controlling\n",
    "    \n",
    "    print(keys[0], keys[0].shape)\n",
    "    rho  = jax.random.multivariate_normal(keys[0], beta_mean, beta_cov) # sample rho'\n",
    "    _rho = jax.random.multivariate_normal(keys[1], beta_mean, beta_cov) # sample rho''\n",
    "    \n",
    "    like  = igw_like(rho, x, beta_cov, a) # calc like scores rho' #alpha, \n",
    "    _like = igw_like(_rho, x, beta_cov, a) # calc like scores rho'' #, alpha\n",
    "    \n",
    "    cond  = _like - like > np.log(jax.random.uniform(keys[2])) # Metropolis Hastings\n",
    "    return jax.lax.select(cond, _rho, rho) # _rho if cond true rho otherwise\n",
    "\n",
    "_sample_rhos_igw = jax.vmap(_sample_rhos_igw)\n",
    "\n",
    "def _sample_rs_init(rhox, key):\n",
    "    '''\n",
    "    expected reward for each action at the last time step T-1 + add gaussian noise and return\n",
    "    '''\n",
    "    mean = np.einsum('ij,j->i', xs[-1], rhox)\n",
    "    rs = mean + sigma * jax.random.normal(key, shape=(T-1,))\n",
    "    return rs\n",
    "\n",
    "def _sample_rs(rhox, rhos, beta0_y, betas_invN, key):\n",
    "    '''\n",
    "    Lemma 1 sampling\n",
    "    '''\n",
    "    invcov = np.eye(T-1) # initialize for inverse covariance 1/σ2 I\n",
    "    \n",
    "        # outer product of xs, weighted by inverse covariance of believes?  # sum over all time steps 'a' \n",
    "        # cross-product 'bc' and 'de' indices  # inverse covariance weights `cd' indices # \\sum^T_t=2 X^T_{t−1} Σ_t X_{t−1}\n",
    "    invcov = invcov + np.einsum('abc,acd,aed->be', xs, betas_invN[1:], xs) \n",
    "    \n",
    "        # dot product of the last context vectors xs[-1] with the parameters rhox  # (X^T_t \\rhox)\n",
    "    invcov_at_mean = np.einsum('ij,j->i', xs[-1], rhox)\n",
    "    \n",
    "        # 1) product of each context vector with its corresponding parameter estimate adjustment\n",
    "        # 2) weighted sum of these adjustments across all contexts xs and adjusted parameters rhos[1:] - ...\n",
    "        # \\sum^T_t=2 X^T_t-1(\\rho_t - C_t C^{-1}_1 \\mu_1)\n",
    "    invcov_at_mean = invcov_at_mean + np.einsum('ijk,ik->j', xs, rhos[1:] - np.einsum('ijk,k->ij', betas_invN[1:], beta0_y))\n",
    "    cov = np.linalg.inv(invcov)\n",
    "    mean = cov @ invcov_at_mean\n",
    "\n",
    "    rs = jax.random.multivariate_normal(key, mean, cov * sigma**2) # sampling the reward from a multivariate normal\n",
    "    return rs\n",
    "\n",
    "def _sample_optimistic(arg0, arg1):\n",
    "    (rhos, rs, rhox, beta0_y, betas_invN), key = arg0, arg1 # , alpha\n",
    "    keys = jax.random.split(key, 2)\n",
    "        \n",
    "    # Calculate beta mean and covariance\n",
    "    betas_mean = np.einsum('ijk,ik->ij', betas_invN, beta0_y + _betas_y(rs))\n",
    "    betas_cov = betas_invN * sigma**2\n",
    "        \n",
    "    # Sample rhos and rewards using the specified policy\n",
    "    rhos = _sample_rhos_optimistic(betas_mean, betas_cov, data_x, data_a, jax.random.split(keys[0], T)) #policy alpha\n",
    "    rs = _sample_rs(rhox, rhos, beta0_y, betas_invN, keys[1])\n",
    "        \n",
    "    return (rhos, rs, rhox, beta0_y, betas_invN), (rhos, rs)\n",
    "\n",
    "def _sample_softmax(arg0, arg1):\n",
    "    (rhos, rs, rhox, beta0_y, betas_invN), key = arg0, arg1 # , alpha\n",
    "    keys = jax.random.split(key, 2)\n",
    "        \n",
    "    # Calculate beta mean and covariance\n",
    "    betas_mean = np.einsum('ijk,ik->ij', betas_invN, beta0_y + _betas_y(rs))\n",
    "    betas_cov = betas_invN * sigma**2\n",
    "        \n",
    "    # Sample rhos and rewards using the specified policy\n",
    "    rhos = _sample_rhos_softmax(betas_mean, betas_cov, data_x, data_a, jax.random.split(keys[0], T)) #policy alpha\n",
    "    rs = _sample_rs(rhox, rhos, beta0_y, betas_invN, keys[1])\n",
    "    \n",
    "    return (rhos, rs, rhox, beta0_y, betas_invN), (rhos, rs)\n",
    "    \n",
    "\n",
    "def _sample_ts(arg0, arg1):\n",
    "    (rhos, rs, rhox, beta0_y, betas_invN), key = arg0, arg1 # , alpha\n",
    "    keys = jax.random.split(key, 2)\n",
    "        \n",
    "    # Calculate beta mean and covariance\n",
    "    betas_mean = np.einsum('ijk,ik->ij', betas_invN, beta0_y + _betas_y(rs))\n",
    "    betas_cov = betas_invN * sigma**2\n",
    "        \n",
    "    # Sample rhos and rewards using the specified policy\n",
    "    rhos = _sample_rhos_ts(betas_mean, betas_cov, data_x, data_a, jax.random.split(keys[0], T)) #policy alpha\n",
    "    rs = _sample_rs(rhox, rhos, beta0_y, betas_invN, keys[1])\n",
    "        \n",
    "    return (rhos, rs, rhox, beta0_y, betas_invN), (rhos, rs)\n",
    "    \n",
    "\n",
    "def _sample_igw(arg0, arg1):\n",
    "    (rhos, rs, rhox, beta0_y, betas_invN), key = arg0, arg1 # , alpha\n",
    "    keys = jax.random.split(key, 2)\n",
    "        \n",
    "    # Calculate beta mean and covariance\n",
    "    betas_mean = np.einsum('ijk,ik->ij', betas_invN, beta0_y + _betas_y(rs))\n",
    "    betas_cov = betas_invN * sigma**2\n",
    "        \n",
    "    # Sample rhos and rewards using the specified policy\n",
    "    rhos = _sample_rhos_igw(betas_mean, betas_cov, data_x, data_a, jax.random.split(keys[0], T)) #policy alpha\n",
    "    rs = _sample_rs(rhox, rhos, beta0_y, betas_invN, keys[1])\n",
    "        \n",
    "    return (rhos, rs, rhox, beta0_y, betas_invN), (rhos, rs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def sample_optimistic(rhox, beta0_y, beta0_N, key): #policy , alpha=20\n",
    "    '''\n",
    "    Orchestration of sampling that leads to sequences of rhos and rewards\n",
    "    '''\n",
    "    keys = jax.random.split(key, 2) # pseudorandom seed keys: 2\n",
    "    \n",
    "    betas_invN = np.linalg.inv(beta0_N + _betas_N) \n",
    "    \n",
    "    rs = _sample_rs_init(rhox, keys[0]) #\n",
    "    \n",
    "    initial =(np.zeros((T,K)), rs, rhox, beta0_y, betas_invN) #, alpha\n",
    "    _, (_RHOS, _RS) = jax.lax.scan(_sample_optimistic, initial, jax.random.split(keys[1], hyper['sample_stop'])) \n",
    "    \n",
    "                \n",
    "    RHOS = _RHOS[hyper['sample_start']::hyper['sample_step']]\n",
    "    RS = _RS[hyper['sample_start']::hyper['sample_step']]\n",
    "    \n",
    "    return RHOS, RS\n",
    "\n",
    "@jax.jit\n",
    "def sample_softmax(rhox, beta0_y, beta0_N, key): #policy , alpha=20\n",
    "    '''\n",
    "    Orchestration of sampling that leads to sequences of rhos and rewards\n",
    "    '''\n",
    "    keys = jax.random.split(key, 2) # pseudorandom seed keys: 2\n",
    "    \n",
    "    betas_invN = np.linalg.inv(beta0_N + _betas_N) \n",
    "    \n",
    "    rs = _sample_rs_init(rhox, keys[0]) #\n",
    "    \n",
    "    initial =(np.zeros((T,K)), rs, rhox, beta0_y, betas_invN) #, alpha\n",
    "    _, (_RHOS, _RS) = jax.lax.scan(_sample_softmax, initial, jax.random.split(keys[1], hyper['sample_stop'])) \n",
    "    \n",
    "                \n",
    "    RHOS = _RHOS[hyper['sample_start']::hyper['sample_step']]\n",
    "    RS = _RS[hyper['sample_start']::hyper['sample_step']]\n",
    "    \n",
    "    return RHOS, RS\n",
    "\n",
    "@jax.jit\n",
    "def sample_ts(rhox, beta0_y, beta0_N, key): #policy , alpha=20\n",
    "    '''\n",
    "    Orchestration of sampling that leads to sequences of rhos and rewards\n",
    "    '''\n",
    "    keys = jax.random.split(key, 2) # pseudorandom seed keys: 2\n",
    "    \n",
    "    betas_invN = np.linalg.inv(beta0_N + _betas_N) \n",
    "    \n",
    "    rs = _sample_rs_init(rhox, keys[0]) #\n",
    "    \n",
    "    initial =(np.zeros((T,K)), rs, rhox, beta0_y, betas_invN) #, alpha\n",
    "    _, (_RHOS, _RS) = jax.lax.scan(_sample_ts, initial, jax.random.split(keys[1], hyper['sample_stop'])) \n",
    "    \n",
    "                \n",
    "    RHOS = _RHOS[hyper['sample_start']::hyper['sample_step']]\n",
    "    RS = _RS[hyper['sample_start']::hyper['sample_step']]\n",
    "    \n",
    "    return RHOS, RS\n",
    "\n",
    "@jax.jit    \n",
    "def sample_igw(rhox, beta0_y, beta0_N, key): #policy , alpha=20\n",
    "    '''\n",
    "    Orchestration of sampling that leads to sequences of rhos and rewards\n",
    "    '''\n",
    "    keys = jax.random.split(key, 2) # pseudorandom seed keys: 2\n",
    "    \n",
    "    betas_invN = np.linalg.inv(beta0_N + _betas_N) \n",
    "    \n",
    "    rs = _sample_rs_init(rhox, keys[0]) #\n",
    "    \n",
    "    initial =(np.zeros((T,K)), rs, rhox, beta0_y, betas_invN) #, alpha\n",
    "    _, (_RHOS, _RS) = jax.lax.scan(_sample_igw, initial, jax.random.split(keys[1], hyper['sample_stop'])) \n",
    "    \n",
    "                \n",
    "    RHOS = _RHOS[hyper['sample_start']::hyper['sample_step']]\n",
    "    RS = _RS[hyper['sample_start']::hyper['sample_step']]\n",
    "    \n",
    "    return RHOS, RS\n",
    "\n",
    "def compute_rhox(RS):\n",
    "    '''\n",
    "    the mean of the product of rewards and contexts over all sampled paths\n",
    "    '''\n",
    "    # updated estimate of the expected rewards conditioned on the actions taken\n",
    "    _beta_y = _BETAS_Y(RS)[:,-1,:].mean(axis=0)\n",
    "    # the last cumulative sum of outer products of contexts\n",
    "    _beta_N = _betas_N[-1]\n",
    "    # solving a linear system \n",
    "    # _beta_N coefficient matrix \n",
    "    # _beta_y vector term \n",
    "    rhox = np.einsum('ij,j->i', np.linalg.inv(_beta_N), _beta_y)\n",
    "    return rhox\n",
    "\n",
    "def _likelihood0(rho, beta_mean, beta_invcov):\n",
    "    '''\n",
    "    Q + logP: calculating log likelihood  | beta_mean beta_invcov\n",
    "    '''\n",
    "    # negative \n",
    "    res = -np.einsum('i,ij,j->', rho-beta_mean, beta_invcov, rho-beta_mean)\n",
    "    res = res + np.log(np.linalg.det(beta_invcov))\n",
    "    return res\n",
    "\n",
    "_likelihood0 = jax.vmap(_likelihood0)\n",
    "\n",
    "def _likelihood1(params, rhos, rs):\n",
    "    '''\n",
    "    Updating posterior beliefs, mean covariance then _likelihood0\n",
    "    '''\n",
    "    beta0_y, beta0_N = decode(params) # initialize parameters\n",
    "    betas_y = beta0_y + _betas_y(rs)\n",
    "    betas_N = beta0_N + _betas_N\n",
    "    betas_invN = np.linalg.inv(betas_N)\n",
    "    betas_mean = np.einsum('ijk,ik->ij', betas_invN, betas_y)\n",
    "    betas_invcov = betas_N / sigma**2\n",
    "    \n",
    "    return _likelihood0(rhos, betas_mean, betas_invcov).sum()\n",
    "\n",
    "_likelihood1 = jax.vmap(_likelihood1, in_axes=(None,0,0))\n",
    "\n",
    "def likelihood(params, RHOS, RS):\n",
    "    '''\n",
    "    Q mean\n",
    "    '''\n",
    "    return _likelihood1(params, RHOS, RS).mean()\n",
    "\n",
    "grad_likelihood = jax.grad(likelihood)\n",
    "grad_likelihood = jax.jit(grad_likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC-BICB RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"C:/Users/huber/Dropbox/ADULTERY/PHD/Cambridge_task/Datasets\"\n",
    "input_dir = \"C:/Users/huber/Dropbox/ADULTERY/PHD/Cambridge_task/Results\"\n",
    "\n",
    "RUNS=10\n",
    "\n",
    "algos = [\"ts\", \"optimistic\", \"softmax\", \"igw\", \"ucb\", \"greedy\"]\n",
    "policies = [\"ts\",\"igw\"] #\"softmax\",  \"optimistic\",\n",
    "policy_map = {\"optimistic\": 0, \"softmax\": 1, \"ts\": 2,  \"igw\": 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 9/9 [2:56:26<00:00, 1176.24s/it]\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "alpha = 20 # exploration parameter in the policy definition\n",
    "sigma = .10 # variance of the rewards\n",
    "\n",
    "for i in tqdm.tqdm(range(1,10)):  # Assuming 100 datasets\n",
    "    for algo in algos:\n",
    "        for policy in policies:\n",
    "            policy_index = policy_map[policy]\n",
    "            filename = f'{output_dir}/dataset_{i}_{algo}.dill'\n",
    "            with open(filename, 'rb') as f:\n",
    "                data = dill.load(f)\n",
    "            \n",
    "                data_x = np.array(data['x'])  \n",
    "                data_a = np.array(data['a'])  \n",
    "                # rhox = data['rhox']           \n",
    "                # betas_mean = data['betas_mean']\n",
    "                # betas_cov=data['betas_cov']\n",
    "\n",
    "                T,A,K = data_x.shape    \n",
    "            \n",
    "                # A dictionary of hyperparameters of the simulation\n",
    "                hyper = dict()\n",
    "                hyper['sample_start'] = 1000\n",
    "                hyper['sample_stop'] = 2000\n",
    "                hyper['sample_step'] = 1\n",
    "                hyper['iter'] = 20\n",
    "\n",
    "                _xs = lambda t0, t1: jax.lax.select(t1 <= t0, data_x[t1,data_a[t1]], np.zeros(K)) \n",
    "                _xs = jax.vmap(jax.vmap(_xs, in_axes=(None,0)), in_axes=(0,None)) \n",
    "                xs = _xs(np.arange(T-1), np.arange(T-1)) \n",
    "                \n",
    "                __betas_N = lambda t: np.einsum('i,j->ij', data_x[t,data_a[t]], data_x[t,data_a[t]])\n",
    "                __betas_N = jax.vmap(__betas_N)\n",
    "                _betas_N = __betas_N(np.arange(T-1)).cumsum(axis=0)\n",
    "                _betas_N = np.concatenate((np.zeros((K,K))[None,...], _betas_N))\n",
    "\n",
    "                __betas_y = lambda r, t: r * data_x[t,data_a[t]]\n",
    "                __betas_y = jax.vmap(__betas_y)\n",
    "                _betas_y = lambda rs: np.concatenate((np.zeros(K)[None,...], __betas_y(rs, np.arange(T-1)).cumsum(axis=0)))\n",
    "                _betas_y = jax.jit(_betas_y)\n",
    "                _BETAS_Y = jax.jit(jax.vmap(_betas_y))\n",
    "            \n",
    "                rhox = -np.ones(K)/K\n",
    "                params = {'beta0': 0.}\n",
    "                grad_mnsq = {'beta0': 0.}\n",
    "                beta0_y, beta0_N = decode(params)\n",
    "\n",
    "                for j in range(hyper['iter']): #tqdm.tqdm(\n",
    "    \n",
    "                    key, subkey = jax.random.split(key)\n",
    "                \n",
    "                    if policy_index == 0:\n",
    "                        RHOS, RS = sample_optimistic(rhox, beta0_y, beta0_N, subkey)\n",
    "                    elif policy_index == 1:\n",
    "                        RHOS, RS = sample_softmax(rhox, beta0_y, beta0_N, subkey)\n",
    "                    elif policy_index == 2:\n",
    "                        RHOS, RS = sample_ts(rhox, beta0_y, beta0_N, subkey)\n",
    "                    elif policy_index == 3:\n",
    "                        RHOS, _RS = sample_igw(rhox, beta0_y, beta0_N, subkey)\n",
    "                    \n",
    "                    rhox = compute_rhox(RS)\n",
    "\n",
    "                    grad = grad_likelihood(params, RHOS, RS)\n",
    "                    grad_mnsq['beta0'] = .1 * grad['beta0']**2 + .9 * grad_mnsq['beta0']\n",
    "                    params['beta0'] += .001 * grad['beta0'] / (np.sqrt(grad_mnsq['beta0']) + 1e-8)\n",
    "                    beta0_y, beta0_N = decode(params)\n",
    "\n",
    "                # print(rhox, beta0_N[0,0])\n",
    "    \n",
    "                rhox = rhox / np.abs(rhox).sum()\n",
    "\n",
    "                res = dict()\n",
    "                res['rhox'] = rhox\n",
    "                res['beta0_y'] = beta0_y\n",
    "                res['beta0_N'] = beta0_N\n",
    "\n",
    "                key, subkey = jax.random.split(key)\n",
    "                if policy_index == 0:\n",
    "                    _, RS = sample_optimistic(rhox, beta0_y, beta0_N, subkey)\n",
    "                elif policy_index == 1:\n",
    "                    _, RS = sample_softmax(rhox, beta0_y, beta0_N, subkey)\n",
    "                elif policy_index == 2:\n",
    "                    _, RS = sample_ts(rhox, beta0_y, beta0_N, subkey)\n",
    "                elif policy_index == 3:\n",
    "                    _, _RS = sample_igw(rhox, beta0_y, beta0_N, subkey)\n",
    "\n",
    "                BETAS_Y = beta0_y + _BETAS_Y(RS)\n",
    "                betas_invN = np.linalg.inv(beta0_N + _betas_N)\n",
    "                betas_mean = np.einsum('ijk,lik->lij', betas_invN, BETAS_Y).mean(axis=0)\n",
    "                betas_cov = betas_invN * sigma**2\n",
    "\n",
    "                res['betas_mean'] = betas_mean\n",
    "                res['betas_cov'] = betas_invN\n",
    "\n",
    "                # Save the results\n",
    "                filename = f'{input_dir}/dataset_{i}_{algo}_{policy}_PCICB.dill'\n",
    "                with open(filename, 'wb') as f:\n",
    "                    dill.dump(res, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC-NBICB UTILITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "import numpy as np1\n",
    "from jax.scipy.special import logsumexp\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jax.config.update('jax_platform_name', 'cpu')\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('-i', '--input', required=True)\n",
    "# parser.add_argument('-o', '--output', default='res/general.obj')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "hyper = dict()\n",
    "hyper['sample_start'] = 10_000\n",
    "hyper['sample_stop'] = 20_000\n",
    "hyper['sample_step'] = 10\n",
    "hyper['variance_rho'] = 5e-4\n",
    "hyper['variance_beta'] = 5e-5\n",
    "hyper['offset'] = -np.ones(K)/K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_cov_T = lambda t0, t1: np.minimum(t0, t1) + 1\n",
    "_cov_T = jax.vmap(jax.vmap(_cov_T, in_axes=(None,0)), in_axes=(0,None))\n",
    "cov_T = _cov_T(np.arange(T), np.arange(T))\n",
    "\n",
    "_cov_K = np.eye(K)\n",
    "_cov_K = _cov_K.at[:,0].set(np.ones(K))\n",
    "_cov_K = _cov_K.at[:,0].set(_cov_K[:,0] / np.sum(_cov_K[:,0]**2)**.5)\n",
    "for i in range(1,K):\n",
    "    for j in range(i):\n",
    "        _cov_K = _cov_K.at[:,i].add(-np.sum(_cov_K[:,i] * _cov_K[:,j]) * _cov_K[:,j])\n",
    "    _cov_K = _cov_K.at[:,i].set(_cov_K[:,i] / np.sum(_cov_K[:,i]**2)**.5)\n",
    "_scale = np.eye(K)\n",
    "_scale = _scale.at[0,0].set(.1)\n",
    "_cov_K = _cov_K @ _scale @ np.linalg.inv(_cov_K)\n",
    "cov_K = _cov_K @ _cov_K.T\n",
    "\n",
    "cov_rho = cov_K * hyper['variance_rho']\n",
    "cov_rhos = np.kron(np.eye(T), cov_K) * hyper['variance_rho']\n",
    "cov_betas = np.kron(cov_T, cov_K) * hyper['variance_beta']\n",
    "invcov_rhos = np.linalg.inv(cov_rhos)\n",
    "invcov_betas = np.linalg.inv(cov_betas)\n",
    "mean_betas = hyper['offset'][None,...].repeat(T, axis=0).reshape(-1)\n",
    "cov = np.linalg.inv(invcov_rhos + invcov_betas)\n",
    "cov_at_invcov_betas_at_mean_betas = cov @ invcov_betas @ mean_betas\n",
    "cov_at_invcov_rhos = cov @ invcov_rhos\n",
    "\n",
    "cov_rho_L = np1.linalg.cholesky(cov_rho)\n",
    "cov_L = np1.linalg.cholesky(cov)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement likelihood functions for each policy\n",
    "def softmax_like(rho, x,  a): #beta_cov,\n",
    "    alpha=20\n",
    "    q = alpha * np.einsum('ij,j->i', x, rho)\n",
    "    \n",
    "    return q[a] - logsumexp(q) \n",
    "    \n",
    "def igw_like(rho, x, a): #beta_cov,\n",
    "    alpha=20\n",
    "    erewards = np.einsum('ij,j->i', x, rho)  # prediction\n",
    "    best_arm = np.argmax(erewards)\n",
    "    gaps = erewards[best_arm] - erewards  # Gaps\n",
    "\n",
    "    A = x.shape[0] \n",
    "    pi = 1 / (A + alpha * gaps)\n",
    "    pi=pi.at[best_arm].set(0) \n",
    "\n",
    "    # Adjust the best arm\n",
    "    pi_best = 1 - np.sum(pi)\n",
    "    pi=pi.at[best_arm].set(pi_best)\n",
    "    \n",
    "    return np.log(pi[a])\n",
    "\n",
    "\n",
    "def _sample_rhos_softmax(beta, x, a, key): #beta_cov,\n",
    "    keys = jax.random.split(key, 3)\n",
    "    rho = beta + cov_rho_L @ jax.random.normal(keys[0], shape=(K,))\n",
    "    _rho = beta + cov_rho_L @ jax.random.normal(keys[1], shape=(K,))\n",
    "    like = softmax_like(rho, x, a) #beta_cov,\n",
    "    _like = softmax_like(_rho, x, a) #beta_cov,\n",
    "    cond = _like - like > np.log(jax.random.uniform(keys[2]))\n",
    "    return jax.lax.select(cond, _rho, rho)\n",
    "_sample_rhos_softmax = jax.vmap(_sample_rhos_softmax)\n",
    "\n",
    "\n",
    "def _sample_rhos_igw(beta, x, a, key): #beta_cov,\n",
    "    keys = jax.random.split(key, 3)\n",
    "    rho = beta + cov_rho_L @ jax.random.normal(keys[0], shape=(K,))\n",
    "    _rho = beta + cov_rho_L @ jax.random.normal(keys[1], shape=(K,))\n",
    "    like = igw_like(rho, x, a) #beta_cov,\n",
    "    _like = igw_like(_rho, x,  a) #beta_cov,\n",
    "    cond = _like - like > np.log(jax.random.uniform(keys[2]))\n",
    "    return jax.lax.select(cond, _rho, rho)\n",
    "_sample_rhos_igw = jax.vmap(_sample_rhos_igw)\n",
    "\n",
    "\n",
    "def _sample_betas(rhos, key):\n",
    "    mean = cov_at_invcov_betas_at_mean_betas + cov_at_invcov_rhos @ rhos.reshape(-1)\n",
    "    _betas = mean + cov_L @ jax.random.normal(key, shape=(T*K,))\n",
    "    return _betas.reshape(-1,K)\n",
    "\n",
    "def _sample_softmax(arg0, arg1):\n",
    "    (rhos, betas), key = arg0, arg1\n",
    "    keys = jax.random.split(key, 2)\n",
    "    rhos = _sample_rhos_softmax(betas, data_x, data_a, jax.random.split(keys[0], T))\n",
    "    betas = _sample_betas(rhos, keys[1])\n",
    "    return (rhos, betas), (rhos, betas)\n",
    "\n",
    "def _sample_igw(arg0, arg1):\n",
    "    (rhos, betas), key = arg0, arg1\n",
    "    keys = jax.random.split(key, 2)\n",
    "    rhos = _sample_rhos_igw(betas, data_x, data_a, jax.random.split(keys[0], T))\n",
    "    betas = _sample_betas(rhos, keys[1])\n",
    "    return (rhos, betas), (rhos, betas)\n",
    "\n",
    "def sample_softmax(rhos, betas, key, count):\n",
    "    (rhos, betas), (RHOS, BETAS) = jax.lax.scan(_sample_softmax, (rhos, betas), jax.random.split(key, count))\n",
    "    return rhos, betas, RHOS, BETAS\n",
    "sample_softmax = jax.jit(sample_softmax, static_argnums=3)\n",
    "\n",
    "def sample_igw(rhos, betas, key, count):\n",
    "    (rhos, betas), (RHOS, BETAS) = jax.lax.scan(_sample_igw, (rhos, betas), jax.random.split(key, count))\n",
    "    return rhos, betas, RHOS, BETAS\n",
    "sample_igw = jax.jit(sample_igw, static_argnums=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC-NBICB RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#algos = [\"softmax\", \"igw\", \"optimistic\", \"ucb\", \"ts\"]\n",
    "algos = [\"greedy\"]\n",
    "policies = [\"igw\"] #\"softmax\"\n",
    "policy_map = {\"optimistic\": 0, \"softmax\": 1, \"ts\": 2,  \"igw\": 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greedy 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████▎                                                                          | 1/10 [00:47<07:11, 47.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.23941611 -0.26060665 -0.25816903 -0.24180819]\n",
      " [-0.22404097 -0.26649195 -0.2616108  -0.2478563 ]\n",
      " [-0.20828402 -0.271469   -0.26413536 -0.25611162]\n",
      " ...\n",
      " [ 0.25112474 -0.33944878 -0.08910514 -0.32032132]\n",
      " [ 0.25095668 -0.33958814 -0.0888503  -0.3206049 ]\n",
      " [ 0.250948   -0.3395229  -0.08881148 -0.32071754]]\n",
      "greedy 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▌                                                                  | 2/10 [01:38<06:35, 49.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.24130087 -0.2607189  -0.25996354 -0.23801665]\n",
      " [-0.22542223 -0.26359338 -0.2623589  -0.24862546]\n",
      " [-0.20995812 -0.26745006 -0.26532215 -0.2572697 ]\n",
      " ...\n",
      " [ 0.26957262 -0.2008628  -0.15042719 -0.3791373 ]\n",
      " [ 0.2695559  -0.20093472 -0.15043432 -0.3790751 ]\n",
      " [ 0.2695856  -0.2011073  -0.15022251 -0.37908465]]\n",
      "greedy 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████▉                                                          | 3/10 [02:29<05:52, 50.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.24069317 -0.2620011  -0.258943   -0.23836279]\n",
      " [-0.22524428 -0.26767987 -0.26144502 -0.24563082]\n",
      " [-0.2094872  -0.27210027 -0.26272002 -0.25569248]\n",
      " ...\n",
      " [ 0.2859512  -0.32933033 -0.10572312 -0.27899536]\n",
      " [ 0.28594074 -0.32934213 -0.10565412 -0.27906302]\n",
      " [ 0.28590712 -0.32928005 -0.10572741 -0.27908543]]\n",
      "greedy 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████▏                                                 | 4/10 [03:19<05:01, 50.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.23976469 -0.26068446 -0.2572771  -0.24227378]\n",
      " [-0.225028   -0.26710796 -0.2606368  -0.24722724]\n",
      " [-0.20868438 -0.27131972 -0.2621783  -0.25781757]\n",
      " ...\n",
      " [ 0.24602117 -0.46581617 -0.05001359 -0.238149  ]\n",
      " [ 0.24600615 -0.4658581  -0.04999042 -0.23814537]\n",
      " [ 0.24598584 -0.46613285 -0.04995407 -0.23792727]]\n",
      "greedy 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████▌                                         | 5/10 [04:12<04:15, 51.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.24011941 -0.2602844  -0.25829187 -0.24130434]\n",
      " [-0.2245218  -0.26532164 -0.26132318 -0.2488334 ]\n",
      " [-0.20878279 -0.26917517 -0.26389876 -0.25814325]\n",
      " ...\n",
      " [ 0.25039747 -0.29715514 -0.13681884 -0.31562862]\n",
      " [ 0.25030965 -0.297239   -0.1368036  -0.31564772]\n",
      " [ 0.25034314 -0.2973754  -0.1367659  -0.31551552]]\n",
      "greedy 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████▊                                 | 6/10 [05:02<03:22, 50.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.24047603 -0.26128483 -0.25848058 -0.2397586 ]\n",
      " [-0.2251692  -0.26710272 -0.2612894  -0.24643873]\n",
      " [-0.2089535  -0.27115464 -0.26306906 -0.25682276]\n",
      " ...\n",
      " [ 0.263715   -0.33751833 -0.11441308 -0.2843536 ]\n",
      " [ 0.2636678  -0.3376225  -0.1143425  -0.2843672 ]\n",
      " [ 0.26369575 -0.3376672  -0.11426371 -0.28437337]]\n",
      "greedy 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████████████████████                         | 7/10 [05:49<02:28, 49.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.23995888 -0.26132095 -0.25734863 -0.24137154]\n",
      " [-0.22493526 -0.26824108 -0.26049522 -0.2463284 ]\n",
      " [-0.20855261 -0.27291042 -0.26143408 -0.25710294]\n",
      " ...\n",
      " [ 0.25350836 -0.43805707 -0.0792273  -0.22920725]\n",
      " [ 0.25349596 -0.43800965 -0.0793397  -0.22915465]\n",
      " [ 0.25349778 -0.43809935 -0.07933308 -0.22906977]]\n",
      "greedy 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████▍                | 8/10 [06:40<01:39, 49.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.23959437 -0.2601576  -0.25650966 -0.2437384 ]\n",
      " [-0.22489874 -0.26640117 -0.26001135 -0.24868874]\n",
      " [-0.20871043 -0.2707454  -0.2617703  -0.2587739 ]\n",
      " ...\n",
      " [ 0.22272862 -0.47086045 -0.06089786 -0.24551305]\n",
      " [ 0.22268246 -0.4708727  -0.06071983 -0.24572502]\n",
      " [ 0.22266011 -0.47109696 -0.06062872 -0.24561416]]\n",
      "greedy 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|██████████████████████████████████████████████████████████████████████████▋        | 9/10 [07:29<00:49, 49.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.23991638 -0.26023716 -0.2576037  -0.2422428 ]\n",
      " [-0.2248919  -0.2661596  -0.26105693 -0.24789162]\n",
      " [-0.2093277  -0.2702507  -0.26303324 -0.25738838]\n",
      " ...\n",
      " [ 0.244986   -0.35565627 -0.11614183 -0.28321594]\n",
      " [ 0.24497396 -0.35595146 -0.11604501 -0.28302953]\n",
      " [ 0.24497415 -0.35597414 -0.11601222 -0.28303945]]\n",
      "greedy 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [08:18<00:00, 49.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.23942623 -0.26006606 -0.25710058 -0.24340713]\n",
      " [-0.22486173 -0.26658848 -0.26052797 -0.24802186]\n",
      " [-0.20865345 -0.27107632 -0.26215595 -0.25811428]\n",
      " ...\n",
      " [ 0.23945545 -0.42183226 -0.10007455 -0.23863778]\n",
      " [ 0.23940662 -0.4217413  -0.10014167 -0.23871039]\n",
      " [ 0.23933457 -0.4219046  -0.09998736 -0.23877355]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "alpha = 20 # exploration parameter in the policy definition\n",
    "sigma = .10 # variance of the rewards\n",
    "\n",
    "for i in tqdm.tqdm(range(RUNS)):  # Assuming 100 datasets\n",
    "    for algo in algos:\n",
    "        for policy in policies:\n",
    "            policy_index = policy_map[policy]\n",
    "            print(algo, policy_index)\n",
    "            filename = f'{output_dir}/dataset_{i}_{algo}.dill'\n",
    "            with open(filename, 'rb') as f:\n",
    "                data = dill.load(f)\n",
    "            \n",
    "                data_x = np.array(data['x'])  \n",
    "                data_a = np.array(data['a'])  \n",
    "                rhox = data['rhox']           \n",
    "                betas_mean = data['betas_mean']\n",
    "                betas_cov=data['betas_cov']\n",
    "\n",
    "                T,A,K = data_x.shape    \n",
    "                \n",
    "                _cov_T = lambda t0, t1: np.minimum(t0, t1) + 1\n",
    "                _cov_T = jax.vmap(jax.vmap(_cov_T, in_axes=(None,0)), in_axes=(0,None))\n",
    "                cov_T = _cov_T(np.arange(T), np.arange(T))\n",
    "\n",
    "                _cov_K = np.eye(K)\n",
    "                _cov_K = _cov_K.at[:,0].set(np.ones(K))\n",
    "                _cov_K = _cov_K.at[:,0].set(_cov_K[:,0] / np.sum(_cov_K[:,0]**2)**.5)\n",
    "                for s in range(1,K):\n",
    "                    for k in range(s):\n",
    "                        _cov_K = _cov_K.at[:,s].add(-np.sum(_cov_K[:,s] * _cov_K[:,k]) * _cov_K[:,k])\n",
    "                        _cov_K = _cov_K.at[:,s].set(_cov_K[:,s] / np.sum(_cov_K[:,s]**2)**.5)\n",
    "                _scale = np.eye(K)\n",
    "                _scale = _scale.at[0,0].set(.1)\n",
    "                _cov_K = _cov_K @ _scale @ np.linalg.inv(_cov_K)\n",
    "                cov_K = _cov_K @ _cov_K.T\n",
    "\n",
    "                cov_rho = cov_K * hyper['variance_rho']\n",
    "                cov_rhos = np.kron(np.eye(T), cov_K) * hyper['variance_rho']\n",
    "                cov_betas = np.kron(cov_T, cov_K) * hyper['variance_beta']\n",
    "                invcov_rhos = np.linalg.inv(cov_rhos)\n",
    "                invcov_betas = np.linalg.inv(cov_betas)\n",
    "                mean_betas = hyper['offset'][None,...].repeat(T, axis=0).reshape(-1)\n",
    "                cov = np.linalg.inv(invcov_rhos + invcov_betas)\n",
    "                cov_at_invcov_betas_at_mean_betas = cov @ invcov_betas @ mean_betas\n",
    "                cov_at_invcov_rhos = cov @ invcov_rhos\n",
    "\n",
    "                cov_rho_L = np1.linalg.cholesky(cov_rho)\n",
    "                cov_L = np1.linalg.cholesky(cov)\n",
    "                \n",
    "                rhos = np.zeros((T,K))\n",
    "                betas = np.zeros((T,K)) + hyper['offset']\n",
    "\n",
    "                BETAS = np.zeros((0,T,K))\n",
    "                for z in range(hyper['sample_stop'] // 200): #tqdm.tqdm( , unit_scale=200)\n",
    "                    key, subkey = jax.random.split(key)\n",
    "                    if policy_index == 1:\n",
    "                        rhos, betas, _RHOS, _BETAS  = sample_softmax(rhos, betas, subkey, 200)\n",
    "                    elif policy_index == 3:\n",
    "                        rhos, betas, _RHOS, _BETAS  = sample_igw(rhos, betas, subkey, 200)                \n",
    "                    \n",
    "#                rhos, betas, _RHOS, _BETAS = sample(rhos, betas, subkey, 200)\n",
    "                    BETAS = np.concatenate((BETAS, _BETAS))\n",
    "\n",
    "                betas = BETAS[hyper['sample_start']::hyper['sample_step']].mean(axis=0)\n",
    "                betas = betas / np.abs(betas).sum(axis=-1, keepdims=True)\n",
    "\n",
    "                res = dict()\n",
    "                res['betas'] = betas\n",
    "                \n",
    "                print(betas)\n",
    "                \n",
    "                output = f'{input_dir}/dataset_{i}_{algo}_{policy}_PCNBICB.dill'\n",
    "                with open(output, 'wb') as f:\n",
    "                    dill.dump(res, f)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'betas': Array([[-0.23648252, -0.2549834 , -0.25364617, -0.25488794],\n",
       "        [-0.22289003, -0.2602487 , -0.25705463, -0.2598066 ],\n",
       "        [-0.20957328, -0.2651861 , -0.2606942 , -0.26454642],\n",
       "        [-0.19630934, -0.2698891 , -0.26354352, -0.2702581 ],\n",
       "        [-0.1838874 , -0.27484503, -0.2673433 , -0.27392417],\n",
       "        [-0.17134437, -0.27950135, -0.27049285, -0.27866143],\n",
       "        [-0.15903208, -0.28454483, -0.27336025, -0.28306282],\n",
       "        [-0.1464307 , -0.28899592, -0.27699545, -0.28757796],\n",
       "        [-0.13425006, -0.29396236, -0.27991995, -0.29186764],\n",
       "        [-0.12192728, -0.29852554, -0.28316343, -0.2963838 ],\n",
       "        [-0.1110523 , -0.3029864 , -0.28615808, -0.2998032 ],\n",
       "        [-0.09864198, -0.30718237, -0.289412  , -0.30476367],\n",
       "        [-0.08795324, -0.3114569 , -0.2922193 , -0.30837053],\n",
       "        [-0.07632723, -0.31552696, -0.29481173, -0.31333405],\n",
       "        [-0.06705999, -0.31896666, -0.29736364, -0.3166097 ],\n",
       "        [-0.05656556, -0.3230236 , -0.30024797, -0.32016286],\n",
       "        [-0.04774893, -0.32632628, -0.30223632, -0.32368845],\n",
       "        [-0.03852113, -0.3303177 , -0.30490047, -0.32626072],\n",
       "        [-0.03062164, -0.33328217, -0.30692312, -0.32917303],\n",
       "        [-0.02197621, -0.33632663, -0.30889142, -0.33280578],\n",
       "        [-0.01462458, -0.33939373, -0.31004915, -0.33593258],\n",
       "        [-0.00727064, -0.34200177, -0.31258455, -0.33814305],\n",
       "        [-0.00038224, -0.3448203 , -0.31383687, -0.34096056],\n",
       "        [ 0.00580014, -0.3433962 , -0.31201246, -0.3387912 ],\n",
       "        [ 0.01160929, -0.3415354 , -0.30987957, -0.33697575],\n",
       "        [ 0.01739145, -0.33979067, -0.3078486 , -0.3349693 ],\n",
       "        [ 0.02268572, -0.33817378, -0.3053676 , -0.333773  ],\n",
       "        [ 0.02742567, -0.3370195 , -0.30357766, -0.3319772 ],\n",
       "        [ 0.03226939, -0.3353277 , -0.30147976, -0.3309231 ],\n",
       "        [ 0.03623014, -0.33413666, -0.29956272, -0.33007047],\n",
       "        [ 0.04046338, -0.33310673, -0.29786706, -0.32856283],\n",
       "        [ 0.04379512, -0.33221442, -0.29642105, -0.32756945],\n",
       "        [ 0.04779398, -0.3311795 , -0.29472086, -0.32630566],\n",
       "        [ 0.05173631, -0.3300436 , -0.2932539 , -0.32496625],\n",
       "        [ 0.05464279, -0.32888356, -0.29212844, -0.32434523],\n",
       "        [ 0.05741332, -0.32848373, -0.2907917 , -0.3233113 ],\n",
       "        [ 0.05955292, -0.32845935, -0.28982267, -0.32216507],\n",
       "        [ 0.06135622, -0.32796317, -0.28901827, -0.32166237],\n",
       "        [ 0.06343798, -0.32753092, -0.2883542 , -0.32067683],\n",
       "        [ 0.06537837, -0.32706544, -0.28731567, -0.32024056],\n",
       "        [ 0.0675431 , -0.32671043, -0.2864696 , -0.31927678],\n",
       "        [ 0.06931853, -0.3264045 , -0.28591627, -0.31836066],\n",
       "        [ 0.07104526, -0.32609522, -0.28499952, -0.31785995],\n",
       "        [ 0.07282263, -0.32606396, -0.28417858, -0.3169348 ],\n",
       "        [ 0.07428802, -0.3260893 , -0.28350732, -0.31611538],\n",
       "        [ 0.07615   , -0.3253396 , -0.28271568, -0.31579477],\n",
       "        [ 0.07764246, -0.3251796 , -0.28191912, -0.3152588 ],\n",
       "        [ 0.07955774, -0.32501528, -0.28111517, -0.31431186],\n",
       "        [ 0.08116138, -0.32501328, -0.2805128 , -0.31331253],\n",
       "        [ 0.08288461, -0.32438943, -0.27993476, -0.3127912 ],\n",
       "        [ 0.08436224, -0.32477313, -0.2788773 , -0.31198737],\n",
       "        [ 0.08558442, -0.32479626, -0.27801448, -0.31160483],\n",
       "        [ 0.08704389, -0.32457912, -0.27712673, -0.3112502 ],\n",
       "        [ 0.08850083, -0.32457498, -0.27660707, -0.31031716],\n",
       "        [ 0.08909433, -0.3244621 , -0.27618828, -0.31025535],\n",
       "        [ 0.08980474, -0.32449588, -0.2758255 , -0.3098739 ],\n",
       "        [ 0.09071764, -0.324517  , -0.2752884 , -0.30947697],\n",
       "        [ 0.09133897, -0.3248061 , -0.27461   , -0.3092449 ],\n",
       "        [ 0.09244204, -0.32487363, -0.27365813, -0.3090262 ],\n",
       "        [ 0.09353318, -0.32494166, -0.2727536 , -0.30877152],\n",
       "        [ 0.09454016, -0.32488778, -0.2721658 , -0.30840632],\n",
       "        [ 0.09533071, -0.3251012 , -0.27170002, -0.307868  ],\n",
       "        [ 0.0959859 , -0.32511145, -0.27122453, -0.3076782 ],\n",
       "        [ 0.09685175, -0.32510656, -0.27079254, -0.30724907],\n",
       "        [ 0.09768775, -0.32526666, -0.27011392, -0.30693167],\n",
       "        [ 0.09834605, -0.3253808 , -0.26950368, -0.30676952],\n",
       "        [ 0.09868736, -0.32532942, -0.26925743, -0.30672574],\n",
       "        [ 0.09874035, -0.3259482 , -0.26883215, -0.30647922],\n",
       "        [ 0.09911222, -0.3261931 , -0.2684253 , -0.3062694 ],\n",
       "        [ 0.09915439, -0.32653514, -0.26806465, -0.3062458 ],\n",
       "        [ 0.09927388, -0.32662216, -0.26788047, -0.30622348],\n",
       "        [ 0.09921327, -0.32699272, -0.2675109 , -0.30628312],\n",
       "        [ 0.09895408, -0.32735077, -0.26730108, -0.306394  ],\n",
       "        [ 0.09906802, -0.32759926, -0.26694775, -0.30638498],\n",
       "        [ 0.09901045, -0.32784382, -0.26664504, -0.30650067],\n",
       "        [ 0.09922532, -0.32793006, -0.26641834, -0.3064263 ],\n",
       "        [ 0.09931881, -0.3281021 , -0.26631483, -0.30626428],\n",
       "        [ 0.0995964 , -0.3282108 , -0.2659584 , -0.30623442],\n",
       "        [ 0.09966414, -0.3283012 , -0.2656971 , -0.30633757],\n",
       "        [ 0.09992968, -0.32816115, -0.26553667, -0.3063725 ],\n",
       "        [ 0.09983823, -0.32842174, -0.2652225 , -0.3065176 ],\n",
       "        [ 0.09999355, -0.32856038, -0.265088  , -0.30635807],\n",
       "        [ 0.10039046, -0.3286382 , -0.26491433, -0.30605698],\n",
       "        [ 0.10042273, -0.3289323 , -0.2647458 , -0.30589914],\n",
       "        [ 0.1006463 , -0.32886952, -0.26467383, -0.30581036],\n",
       "        [ 0.10083835, -0.3285491 , -0.26471558, -0.30589697],\n",
       "        [ 0.10073012, -0.32888487, -0.2645007 , -0.30588433],\n",
       "        [ 0.10083713, -0.3290128 , -0.26428586, -0.30586416],\n",
       "        [ 0.10105049, -0.32913864, -0.263924  , -0.3058869 ],\n",
       "        [ 0.10105295, -0.32929713, -0.26396373, -0.30568618],\n",
       "        [ 0.10111196, -0.3291654 , -0.26386878, -0.30585384],\n",
       "        [ 0.10115027, -0.32928088, -0.26354063, -0.3060283 ],\n",
       "        [ 0.10106789, -0.3294814 , -0.26357755, -0.30587313],\n",
       "        [ 0.10123111, -0.3293788 , -0.26347864, -0.3059114 ],\n",
       "        [ 0.10132699, -0.32954928, -0.26334935, -0.3057744 ],\n",
       "        [ 0.10149653, -0.32940322, -0.2635454 , -0.3055549 ],\n",
       "        [ 0.10157628, -0.3291798 , -0.26340014, -0.3058438 ],\n",
       "        [ 0.10142012, -0.32914144, -0.26333252, -0.3061059 ],\n",
       "        [ 0.10142954, -0.32914194, -0.26319686, -0.3062317 ],\n",
       "        [ 0.10134313, -0.32918662, -0.26307428, -0.30639592]],      dtype=float32)}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
